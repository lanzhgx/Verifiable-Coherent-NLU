{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL9AU7zTgP9n"
   },
   "source": [
    "### **Toward Consistent, Verifiable, and Coherent Commonsense Reasoning in Large LMs**\n",
    "\n",
    "This notebook provides source code for our two papers in Findings of EMNLP 2021:\n",
    "\n",
    "\n",
    "1.  Shane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Y. Chai (2021). *Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding.* Findings of EMNLP 2021.\n",
    "2.   Shane Storks and Joyce Y. Chai (2021). *Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers.* Findings of EMNLP 2021.\n",
    "\n",
    "*If you have any questions or problems, please open an issue on our [GitHub repo](https://github.com/sled-group/Verifiable-Coherent-NLU) or email Shane Storks.*\n",
    "\n",
    "***First, configure the execution mode by selecting a few settings (expand cell if needed):***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct4cd2_TFYDk"
   },
   "source": [
    "   0. (Colab only) Insert the path in your Google Drive to the folder where this notebook is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vq9-dXJXFWh3"
   },
   "outputs": [],
   "source": [
    "DRIVE_PATH = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxzL0hhHAHzN"
   },
   "source": [
    "1.   Model type (choose from BERT large, RoBERTa large, RoBERTa large + MNLI, DeBERTa base, and DeBERTa large).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FFNe6vlTaHsP"
   },
   "outputs": [],
   "source": [
    "#mode = 'bert' # BERT large\n",
    "mode = 'roberta' # RoBERTa large\n",
    "# mode = 'roberta_mnli' # RoBERTa large pre-trained on MNLI\n",
    "# mode = 'deberta' # DeBERTa base for training on TRIP\n",
    "# mode = 'deberta_large' # DeBERTa large for training on CE and ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbJ-XeY1aCpD"
   },
   "source": [
    "2.   Name of the task we want to train or evaluate on. Set `debug` to `True` to run quick training/evaluation jobs on only a small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iAQGu6JMa-o6"
   },
   "outputs": [],
   "source": [
    "task_name = 'trip'\n",
    "# task_name = 'ce'\n",
    "# task_name = 'art'\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoWKXfQBd435"
   },
   "source": [
    "3.   (If training models) Training batch size, learning rate, and maximum number of epochs. Settings for results in the paper are provided as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UyFFcZtkeKwT"
   },
   "outputs": [],
   "source": [
    "config_batch_size = 1\n",
    "config_lr = 1e-5 # Selected learning rate for best RoBERTa-based model in TRIP paper\n",
    "config_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH9a70CTaGpG"
   },
   "source": [
    "4.   (For training TRIP models only) Configure the loss weighting scheme for training models here. We provide the 4 modes from the paper as examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UvQEiNBSACak"
   },
   "outputs": [],
   "source": [
    "# Loss weights for (attributes, preconditions, effects, conflicts, story choices)\n",
    "if task_name != 'trip':\n",
    "  print(\"We do not need a loss weighting scheme for %s dataset. Ignoring this cell.\" % task_name)\n",
    "# loss_weights = [0.0, 0.4, 0.4, 0.1, 0.1] # \"All losses\"\n",
    "loss_weights = [0.0, 0.4, 0.4, 0.2, 0.0] # \"Omit story choice loss\"\n",
    "# loss_weights = [0.0, 0.4, 0.4, 0.0, 0.2] # \"Omit conflict detection loss\"\n",
    "# loss_weights = [0.0, 0.0, 0.0, 0.5, 0.5] # \"Omit state classification losses\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmpchQTIg3HZ"
   },
   "source": [
    "   5. (If evaluating models) Provide the name of the pre-trained model directory here. This should be the name of a directory within the *saved_models* directory, which should be located where this notebook is. Names of provided pre-trained model directories are listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "W8tH7UMZhI1N"
   },
   "outputs": [],
   "source": [
    "# TRIP, all losses\n",
    "# eval_model_dir = 'bert-large-uncased_cloze_1_5e-06_4_0.0-0.4-0.4-0.1-0.1_tiered_pipeline_ablate_attributes_states-logits'\n",
    "# eval_model_dir = 'roberta-large_cloze_1_1e-05_7_0.0-0.4-0.4-0.1-0.1_tiered_pipeline_ablate_attributes_states-logits'\n",
    "# eval_model_dir = 'microsoft-deberta-base_cloze_1_5e-06_5_0.0-0.4-0.4-0.1-0.1_tiered_pipeline_ablate_attributes_states-logits'\n",
    "\n",
    "# TRIP, no story classification loss\n",
    "# eval_model_dir = 'bert-large-uncased_cloze_1_5e-05_8_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_ablate_attributes_states-logits'\n",
    "#eval_model_dir = 'roberta-large_cloze_1_1e-05_5_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits' # Best model trained in the TRIP paper\n",
    "# eval_model_dir = 'microsoft-deberta-base_cloze_1_5e-05_5_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_ablate_attributes_states-logits'\n",
    "\n",
    "# TRIP, no conflict detection loss\n",
    "# eval_model_dir = 'bert-large-uncased_cloze_1_1e-06_1_0.0-0.4-0.4-0.0-0.2_tiered_pipeline_ablate_attributes_states-logits'\n",
    "# eval_model_dir = 'roberta-large_cloze_1_5e-06_8_0.0-0.4-0.4-0.0-0.2_tiered_pipeline_ablate_attributes_states-logits'\n",
    "# eval_model_dir = 'microsoft-deberta-base_cloze_1_1e-06_3_0.0-0.4-0.4-0.0-0.2_tiered_pipeline_ablate_attributes_states-logits'\n",
    "\n",
    "# TRIP, no physical state classification loss\n",
    "# eval_model_dir = 'bert-large-uncased_cloze_1_1e-05_3_0.0-0.0-0.0-0.5-0.5_tiered_pipeline_ablate_attributes_states-logits'\n",
    "# eval_model_dir = 'roberta-large_cloze_1_1e-06_7_0.0-0.0-0.0-0.5-0.5_tiered_pipeline_ablate_attributes_states-logits'\n",
    "# eval_model_dir = 'microsoft-deberta-base_cloze_1_5e-06_9_0.0-0.0-0.0-0.5-0.5_tiered_pipeline_ablate_attributes_states-logits'\n",
    "\n",
    "# CE\n",
    "# eval_model_dir = 'bert-large-uncased_ConvEnt_32_7.5e-06_7_xval'\n",
    "# eval_model_dir = 'roberta-large_ConvEnt_32_7.5e-06_9_xval'\n",
    "# eval_model_dir = 'roberta-large-mnli_ConvEnt_32_7.5e-06_7_xval'\n",
    "# eval_model_dir = 'microsoft-deberta-large_ConvEnt_16_1e-05_9_xval'\n",
    "\n",
    "# ART\n",
    "# eval_model_dir = 'bert-large-uncased_art_64_5e-06_8'\n",
    "# eval_model_dir = 'roberta-large_art_64_2.5e-06_4'\n",
    "# eval_model_dir = 'DeBERTa-deberta-large_art_32_1e-06_8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GA5VS3Sfgfz"
   },
   "source": [
    "**For more configuration options, scroll down to the Train Models > Configure Hyperparameters cell for the task you're working on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqvj34KhLL0k"
   },
   "source": [
    "# Setup\n",
    "Run this block every time when starting up the notebook. It will get Colab ready, preprocess the data, and load model packages and classes we'll need later. May take several minutes to run for the first time.\n",
    "\n",
    "**If you get a `ModuleNotFoundError` for the `www` code base, try the following:**\n",
    "\n",
    "\n",
    "1.   Ensure the DRIVE_PATH is set properly above.\n",
    "2.   (Colab only) Verify that this notebook has access to your Google Drive (click the folder icon on the left and then the Google Drive icon).\n",
    "2.   Try to restart the runtime and refresh your browser window.\n",
    "2.   (Colab only) If the problem persists, revoke access to Google Drive and re-enable it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm7qzKnAnbU9"
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Enable auto reloading of code libraries from Google Drive, set up connection to Google Drive, and import some packages. üîå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1631304597639,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "U8h8hUVaqySd",
    "outputId": "6cd18c92-af82-4446-81e2-ef91e34d61f8"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./env/lib/python3.6/site-packages (1.19.5)\r\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3429,
     "status": "ok",
     "timestamp": 1631304601236,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "P3dNQeNNnkHD",
    "outputId": "a9eaffd8-a622-496b-c5e0-80d5728c382b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in ./env/lib/python3.6/site-packages (3.0.0)\r\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./env/lib/python3.6/site-packages (from jsonlines) (21.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.6/site-packages (from jsonlines) (3.10.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "!{sys.executable} -m pip install jsonlines\n",
    "\n",
    "sys.path.append(DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQLB_Y-wSsfk"
   },
   "source": [
    "## Model Setup\n",
    "\n",
    "Next, we'll load up the transformer model, tokenizer, etc. ‚è≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoY37xIF-oP7"
   },
   "source": [
    "### Install HuggingFace transformers and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13405,
     "status": "ok",
     "timestamp": 1631304614620,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "_rp3vUVjT9I4",
    "outputId": "639357f3-18e8-4ab1-f382-073891b24621",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%pip install 'transformers==4.2.2'\\n%pip install sentencepiece\\n%pip install --upgrade setuptools\\n%pip install --upgrade pip\\n%pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\\n%pip install Cython\\n%pip install deberta\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first time\n",
    "'''\n",
    "%pip install 'transformers==4.2.2'\n",
    "%pip install sentencepiece\n",
    "%pip install --upgrade setuptools\n",
    "%pip install --upgrade pip\n",
    "%pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\n",
    "%pip install Cython\n",
    "%pip install deberta\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipywidgets\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4LFuLhzAa2j"
   },
   "source": [
    "### Get Model Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZhhgV9c__TU"
   },
   "source": [
    "Specify which model parameters from transformers we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uA6XunCb_gd9"
   },
   "outputs": [],
   "source": [
    "if task_name in ['trip', 'ce']:\n",
    "  multiple_choice = False\n",
    "elif task_name == 'art':\n",
    "  multiple_choice = True\n",
    "else:\n",
    "  raise ValueError(\"Task name should be set to 'trip', 'ce', or 'art' in the first cell of the notebook!\")\n",
    "\n",
    "if mode == 'bert':\n",
    "  model_name = 'bert-large-uncased'\n",
    "elif mode == 'roberta':\n",
    "  model_name = 'roberta-large'\n",
    "elif mode == 'roberta_mnli':\n",
    "  model_name = 'roberta-large-mnli'\n",
    "elif mode == 'deberta':\n",
    "  model_name = 'microsoft/deberta-base'\n",
    "elif mode == 'deberta_large':\n",
    "  model_name = 'microsoft/deberta-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgFIl1MJ-185"
   },
   "source": [
    "Load the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "etkxf75f-9Gj"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer, DebertaTokenizer, AlbertTokenizer, T5Tokenizer, GPT2Tokenizer\n",
    "\n",
    "from DeBERTa import deberta\n",
    "if mode in ['bert']:\n",
    "  tokenizer_class = BertTokenizer\n",
    "elif mode in ['roberta', 'roberta_mnli']:\n",
    "  tokenizer_class = RobertaTokenizer\n",
    "elif mode in ['deberta', 'deberta_large']:\n",
    "  tokenizer_class = DebertaTokenizer\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name, \n",
    "                                                do_lower_case = False, \n",
    "                                                cache_dir=os.path.join(DRIVE_PATH, 'cache'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0iYZG6bBGIf"
   },
   "source": [
    "Load the model and optimizer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1PxFghcDBPm_"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, RobertaForSequenceClassification, DebertaForSequenceClassification, AlbertForSequenceClassification, AdamW\n",
    "from transformers import BertForMultipleChoice, RobertaForMultipleChoice, AlbertForMultipleChoice, DebertaModel\n",
    "from transformers import BertModel, RobertaModel, AlbertModel, DebertaModel, T5Model, T5EncoderModel, GPT2Model\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import BertConfig, RobertaConfig, DebertaConfig, AlbertConfig, T5Config, GPT2Config\n",
    "from www.model.transformers_ext import DebertaForMultipleChoice\n",
    "from torch.optim import Adam\n",
    "if not multiple_choice:\n",
    "  if mode == 'bert':\n",
    "    model_class = BertForSequenceClassification\n",
    "    config_class = BertConfig\n",
    "    emb_class = BertModel\n",
    "  elif mode in ['roberta', 'roberta_mnli']:\n",
    "    model_class = RobertaForSequenceClassification\n",
    "    config_class = RobertaConfig\n",
    "    emb_class = RobertaModel\n",
    "    lm_class = RobertaForMaskedLM\n",
    "  elif mode in ['deberta', 'deberta_large']:\n",
    "    model_class = DebertaForSequenceClassification\n",
    "    config_class = DebertaConfig\n",
    "    emb_class = DebertaModel\n",
    "else:\n",
    "  if mode == 'bert':\n",
    "    model_class = BertForMultipleChoice\n",
    "    config_class = BertConfig\n",
    "    emb_class = BertModel    \n",
    "  elif mode in ['roberta', 'roberta_mnli']:\n",
    "    model_class = RobertaForMultipleChoice\n",
    "    config_class = RobertaConfig\n",
    "    emb_class = RobertaModel\n",
    "    lm_class = RobertaForMaskedLM\n",
    "  elif mode in ['deberta', 'deberta_large']:\n",
    "    model_class = DebertaForMultipleChoice\n",
    "    config_class = DebertaConfig\n",
    "    emb_class = DebertaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzFnAVtuUmpQ"
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Preprocess the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKY0hTEgnQgB"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Construct the dataset from the .txt files collected from AMT. Save a backup copy in Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4250,
     "status": "ok",
     "timestamp": 1631304619289,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "aE-LOkJ4nWuu",
    "outputId": "61921ed0-f763-4280-d0ea-5979458d51ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed examples:\n",
      "{\n",
      "  story_id: \n",
      "    13,\n",
      "  worker_id: \n",
      "    A32W24TWSWXW,\n",
      "  type: \n",
      "    None,\n",
      "  idx: \n",
      "    None,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    John,\n",
      "  location: \n",
      "    kitchen,\n",
      "  objects: \n",
      "    cabinet, counter, knife, pan, potato, pizza,\n",
      "  sentences: \n",
      "    [\n",
      "      John was getting the snacks ready for the party.\n",
      "      John opened the cabinet, took out a pan and put it on the counter.\n",
      "      John opened the fridge and got out the pizza.\n",
      "      John put the pizza on the pan and put them into the oven.\n",
      "      John took a knife and cut the hot pizza in eight slices.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    13,\n",
      "  plausible: \n",
      "    True,\n",
      "  breakpoint: \n",
      "    -1,\n",
      "  confl_sents: \n",
      "    [],\n",
      "  confl_pairs: \n",
      "    [],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['knife', 2], ['slices', 2], ['hot pizza', 0]], 'exist': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'clean': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'power': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'functional': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'pieces': [['knife', 0], ['slices', 0], ['hot pizza', 4]], 'wet': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'open': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'temperature': [['knife', 0], ['slices', 0], ['hot pizza', 2]], 'solid': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'contain': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'running': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'moveable': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'mixed': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'edible': [['knife', 0], ['slices', 0], ['hot pizza', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  story_id: \n",
      "    13,\n",
      "  worker_id: \n",
      "    A32W24TWSWXW,\n",
      "  type: \n",
      "    cloze,\n",
      "  idx: \n",
      "    0,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    John,\n",
      "  location: \n",
      "    kitchen,\n",
      "  objects: \n",
      "    cabinet, counter, knife, pan, potato, pizza,\n",
      "  sentences: \n",
      "    [\n",
      "      John was getting the snacks ready for the party.\n",
      "      John opened the cabinet, took out a pan and put it on the counter.\n",
      "      John opened the fridge and got out the pizza.\n",
      "      John put the pizza on the pan and put them into the oven.\n",
      "      John called the pizza joint to deliver a pizza.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    13-C0,\n",
      "  plausible: \n",
      "    False,\n",
      "  breakpoint: \n",
      "    4,\n",
      "  confl_sents: \n",
      "    [\n",
      "      2\n",
      "      3\n",
      "    ],\n",
      "  confl_pairs: \n",
      "    [\n",
      "      [2, 4]\n",
      "      [3, 4]\n",
      "    ],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['pizza', 0]], 'exist': [['pizza', 4]], 'clean': [['pizza', 0]], 'power': [['pizza', 0]], 'functional': [['pizza', 2]], 'pieces': [['pizza', 0]], 'wet': [['pizza', 0]], 'open': [['pizza', 0]], 'temperature': [['pizza', 0]], 'solid': [['pizza', 0]], 'contain': [['pizza', 0]], 'running': [['pizza', 0]], 'moveable': [['pizza', 2]], 'mixed': [['pizza', 0]], 'edible': [['pizza', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  story_id: \n",
      "    13,\n",
      "  worker_id: \n",
      "    A32W24TWSWXW,\n",
      "  type: \n",
      "    order,\n",
      "  idx: \n",
      "    2,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    John,\n",
      "  location: \n",
      "    kitchen,\n",
      "  objects: \n",
      "    cabinet, counter, knife, pan, potato, pizza,\n",
      "  sentences: \n",
      "    [\n",
      "      John was getting the snacks ready for the party.\n",
      "      John opened the cabinet, took out a pan and put it on the counter.\n",
      "      John put the pizza on the pan and put them into the oven.\n",
      "      John opened the fridge and got out the pizza.\n",
      "      John took a knife and cut the hot pizza in eight slices.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    13-O2,\n",
      "  plausible: \n",
      "    False,\n",
      "  breakpoint: \n",
      "    3,\n",
      "  confl_sents: \n",
      "    [\n",
      "      2\n",
      "    ],\n",
      "  confl_pairs: \n",
      "    [],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['knife', 2], ['slices', 2], ['hot pizza', 0]], 'exist': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'clean': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'power': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'functional': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'pieces': [['knife', 0], ['slices', 0], ['hot pizza', 4]], 'wet': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'open': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'temperature': [['knife', 0], ['slices', 0], ['hot pizza', 2]], 'solid': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'contain': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'running': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'moveable': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'mixed': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'edible': [['knife', 0], ['slices', 0], ['hot pizza', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  story_id: \n",
      "    33,\n",
      "  worker_id: \n",
      "    A1F01FVEPYCPHO,\n",
      "  type: \n",
      "    None,\n",
      "  idx: \n",
      "    None,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    Mary,\n",
      "  location: \n",
      "    bathroom,\n",
      "  objects: \n",
      "    washing machine, cabinet, toothpaste, bleach, socks, mirror,\n",
      "  sentences: \n",
      "    [\n",
      "      Mary took off her socks.\n",
      "      Mary put the socks in the washing machine.\n",
      "      Mary opened the cabinet.\n",
      "      Mary took out the toothbrush and toothpaste.\n",
      "      Mary brushed her teeth while looking in the mirror.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    33,\n",
      "  plausible: \n",
      "    True,\n",
      "  breakpoint: \n",
      "    -1,\n",
      "  confl_sents: \n",
      "    [],\n",
      "  confl_pairs: \n",
      "    [],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 8]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['socks', 5]], 'exist': [['socks', 2]], 'clean': [['socks', 0]], 'power': [['socks', 0]], 'functional': [['socks', 2]], 'pieces': [['socks', 0]], 'wet': [['socks', 0]], 'open': [['socks', 0]], 'temperature': [['socks', 0]], 'solid': [['socks', 0]], 'contain': [['socks', 0]], 'running': [['socks', 0]], 'moveable': [['socks', 2]], 'mixed': [['socks', 0]], 'edible': [['socks', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['socks', 6], ['washing machine', 0]], 'exist': [['socks', 2], ['washing machine', 2]], 'clean': [['socks', 0], ['washing machine', 0]], 'power': [['socks', 0], ['washing machine', 0]], 'functional': [['socks', 2], ['washing machine', 2]], 'pieces': [['socks', 0], ['washing machine', 0]], 'wet': [['socks', 0], ['washing machine', 0]], 'open': [['socks', 0], ['washing machine', 8]], 'temperature': [['socks', 0], ['washing machine', 0]], 'solid': [['socks', 0], ['washing machine', 0]], 'contain': [['socks', 0], ['washing machine', 6]], 'running': [['socks', 0], ['washing machine', 0]], 'moveable': [['socks', 2], ['washing machine', 2]], 'mixed': [['socks', 0], ['washing machine', 0]], 'edible': [['socks', 0], ['washing machine', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['cabinet', 0]], 'exist': [['cabinet', 2]], 'clean': [['cabinet', 0]], 'power': [['cabinet', 0]], 'functional': [['cabinet', 2]], 'pieces': [['cabinet', 0]], 'wet': [['cabinet', 0]], 'open': [['cabinet', 4]], 'temperature': [['cabinet', 0]], 'solid': [['cabinet', 0]], 'contain': [['cabinet', 0]], 'running': [['cabinet', 0]], 'moveable': [['cabinet', 2]], 'mixed': [['cabinet', 0]], 'edible': [['cabinet', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['toothbrush', 7], ['toothpaste', 7]], 'exist': [['toothbrush', 2], ['toothpaste', 2]], 'clean': [['toothbrush', 0], ['toothpaste', 0]], 'power': [['toothbrush', 0], ['toothpaste', 0]], 'functional': [['toothbrush', 2], ['toothpaste', 2]], 'pieces': [['toothbrush', 0], ['toothpaste', 0]], 'wet': [['toothbrush', 0], ['toothpaste', 0]], 'open': [['toothbrush', 0], ['toothpaste', 0]], 'temperature': [['toothbrush', 0], ['toothpaste', 0]], 'solid': [['toothbrush', 0], ['toothpaste', 0]], 'contain': [['toothbrush', 0], ['toothpaste', 0]], 'running': [['toothbrush', 0], ['toothpaste', 0]], 'moveable': [['toothbrush', 2], ['toothpaste', 2]], 'mixed': [['toothbrush', 0], ['toothpaste', 0]], 'edible': [['toothbrush', 0], ['toothpaste', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 6]], 'location': [['mirror', 0], ['teeth', 0]], 'exist': [['mirror', 2], ['teeth', 2]], 'clean': [['mirror', 0], ['teeth', 6]], 'power': [['mirror', 0], ['teeth', 0]], 'functional': [['mirror', 2], ['teeth', 2]], 'pieces': [['mirror', 0], ['teeth', 0]], 'wet': [['mirror', 0], ['teeth', 0]], 'open': [['mirror', 0], ['teeth', 0]], 'temperature': [['mirror', 0], ['teeth', 0]], 'solid': [['mirror', 0], ['teeth', 0]], 'contain': [['mirror', 0], ['teeth', 0]], 'running': [['mirror', 0], ['teeth', 0]], 'moveable': [['mirror', 2], ['teeth', 0]], 'mixed': [['mirror', 0], ['teeth', 0]], 'edible': [['mirror', 0], ['teeth', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from www.utils import print_dict\n",
    "\n",
    "partitions = ['train', 'dev', 'test']\n",
    "subtasks = ['cloze', 'order']\n",
    "\n",
    "# We can split the data into multiple json files later\n",
    "data_file = os.path.join(DRIVE_PATH, 'all_data/www.json')\n",
    "with open(data_file, 'r') as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "print('Preprocessed examples:')\n",
    "for ex_idx in [0,1,5,10]:\n",
    "  ex = dataset['dev'][list(dataset['dev'].keys())[ex_idx]]\n",
    "  print_dict(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-twYzY5rF1Mi"
   },
   "outputs": [],
   "source": [
    "cloze_dataset = {p: [] for p in dataset}\n",
    "order_dataset = {p: [] for p in dataset}\n",
    "\n",
    "for p in dataset:\n",
    "  for exid in dataset[p]:\n",
    "    ex = dataset[p][exid]\n",
    "\n",
    "    if ex['type'] == None:\n",
    "      continue\n",
    "    \n",
    "    ex_plaus = dataset[p][str(ex['story_id'])]\n",
    "\n",
    "    if ex['type'] == 'cloze':\n",
    "      cloze_dataset[p].append(ex)\n",
    "      cloze_dataset[p].append(ex_plaus) # For every implausible story, add a copy of its corresponding plausible story\n",
    "\n",
    "    # Exclude augmented ordering examples from dev and test, since the breakpoints aren't always accurate in those\n",
    "    elif ex['type'] == 'order' and not (p != 'train' and ex['aug']): \n",
    "      order_dataset[p].append(ex)\n",
    "      order_dataset[p].append(ex_plaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz5tcmScJrka"
   },
   "source": [
    "\n",
    "\n",
    "### Convert TRIP to Two-Story Classification Task\n",
    "\n",
    "Ready the TRIP dataset for two-story classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3982,
     "status": "ok",
     "timestamp": 1631304623775,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "Af976ygKJv7W",
    "outputId": "8335b25d-0f7b-4404-db5c-2dbc4f65b2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze label distribution (train):\n",
      "[(1, 400), (0, 399)]\n",
      "Cloze label distribution (dev):\n",
      "[(0, 161), (1, 161)]\n",
      "Cloze label distribution (test):\n",
      "[(1, 176), (0, 175)]\n",
      "{\n",
      "  example_id: \n",
      "    0-C0,\n",
      "  stories: \n",
      "    [\n",
      "      {'story_id': 0, 'worker_id': 'A1F01FVEPYCPHO', 'type': 'cloze', 'idx': 0, 'aug': False, 'actor': 'Tom', 'location': 'kitchen', 'objects': 'dustbin, microwave, pan, plate, cereal, soup', 'sentences': ['Tom bought a new dustbin for the kitchen.', 'Tom threw a broken plate in the dustbin.', 'Tom got some soup from the fridge.', 'Tom put the soup in the microwave.', 'Tom ate the cold soup.'], 'length': 5, 'example_id': '0-C0', 'plausible': False, 'breakpoint': 4, 'confl_sents': [3], 'confl_pairs': [[3, 4]], 'states': [{'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 6]], 'exist': [['dustbin', 4]], 'clean': [['dustbin', 0]], 'power': [['dustbin', 0]], 'functional': [['dustbin', 2]], 'pieces': [['dustbin', 0]], 'wet': [['dustbin', 0]], 'open': [['dustbin', 0]], 'temperature': [['dustbin', 0]], 'solid': [['dustbin', 0]], 'contain': [['dustbin', 0]], 'running': [['dustbin', 0]], 'moveable': [['dustbin', 2]], 'mixed': [['dustbin', 0]], 'edible': [['dustbin', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 0], ['plate', 6]], 'exist': [['dustbin', 2], ['plate', 2]], 'clean': [['dustbin', 0], ['plate', 5]], 'power': [['dustbin', 0], ['plate', 0]], 'functional': [['dustbin', 2], ['plate', 1]], 'pieces': [['dustbin', 0], ['plate', 0]], 'wet': [['dustbin', 0], ['plate', 0]], 'open': [['dustbin', 0], ['plate', 0]], 'temperature': [['dustbin', 0], ['plate', 0]], 'solid': [['dustbin', 0], ['plate', 0]], 'contain': [['dustbin', 6], ['plate', 0]], 'running': [['dustbin', 0], ['plate', 0]], 'moveable': [['dustbin', 0], ['plate', 2]], 'mixed': [['dustbin', 0], ['plate', 0]], 'edible': [['dustbin', 0], ['plate', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['fridge', 0], ['soup', 2]], 'exist': [['fridge', 2], ['soup', 2]], 'clean': [['fridge', 0], ['soup', 0]], 'power': [['fridge', 0], ['soup', 0]], 'functional': [['fridge', 2], ['soup', 2]], 'pieces': [['fridge', 0], ['soup', 0]], 'wet': [['fridge', 0], ['soup', 0]], 'open': [['fridge', 8], ['soup', 0]], 'temperature': [['fridge', 0], ['soup', 1]], 'solid': [['fridge', 0], ['soup', 0]], 'contain': [['fridge', 8], ['soup', 0]], 'running': [['fridge', 0], ['soup', 0]], 'moveable': [['fridge', 2], ['soup', 2]], 'mixed': [['fridge', 0], ['soup', 0]], 'edible': [['fridge', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0], ['soup', 3]], 'exist': [['microwave', 2], ['soup', 2]], 'clean': [['microwave', 0], ['soup', 0]], 'power': [['microwave', 2], ['soup', 0]], 'functional': [['microwave', 2], ['soup', 2]], 'pieces': [['microwave', 0], ['soup', 0]], 'wet': [['microwave', 0], ['soup', 0]], 'open': [['microwave', 8], ['soup', 0]], 'temperature': [['microwave', 0], ['soup', 0]], 'solid': [['microwave', 0], ['soup', 0]], 'contain': [['microwave', 6], ['soup', 0]], 'running': [['microwave', 0], ['soup', 0]], 'moveable': [['microwave', 2], ['soup', 2]], 'mixed': [['microwave', 0], ['soup', 0]], 'edible': [['microwave', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['soup', 1]], 'exist': [['soup', 3]], 'clean': [['soup', 0]], 'power': [['soup', 0]], 'functional': [['soup', 2]], 'pieces': [['soup', 0]], 'wet': [['soup', 0]], 'open': [['soup', 0]], 'temperature': [['soup', 7]], 'solid': [['soup', 0]], 'contain': [['soup', 0]], 'running': [['soup', 0]], 'moveable': [['soup', 2]], 'mixed': [['soup', 0]], 'edible': [['soup', 0]]}]}\n",
      "      {'story_id': 0, 'worker_id': 'A1F01FVEPYCPHO', 'type': None, 'idx': None, 'aug': False, 'actor': 'Tom', 'location': 'kitchen', 'objects': 'dustbin, microwave, pan, plate, cereal, soup', 'sentences': ['Tom bought a new dustbin for the kitchen.', 'Tom threw a broken plate in the dustbin.', 'Tom got some soup from the fridge.', 'Tom put the soup in the microwave.', 'Tom turned on the microwave.'], 'length': 5, 'example_id': '0', 'plausible': True, 'breakpoint': -1, 'confl_sents': [], 'confl_pairs': [], 'states': [{'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 6]], 'exist': [['dustbin', 4]], 'clean': [['dustbin', 0]], 'power': [['dustbin', 0]], 'functional': [['dustbin', 2]], 'pieces': [['dustbin', 0]], 'wet': [['dustbin', 0]], 'open': [['dustbin', 0]], 'temperature': [['dustbin', 0]], 'solid': [['dustbin', 0]], 'contain': [['dustbin', 0]], 'running': [['dustbin', 0]], 'moveable': [['dustbin', 2]], 'mixed': [['dustbin', 0]], 'edible': [['dustbin', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 0], ['plate', 6]], 'exist': [['dustbin', 2], ['plate', 2]], 'clean': [['dustbin', 0], ['plate', 5]], 'power': [['dustbin', 0], ['plate', 0]], 'functional': [['dustbin', 2], ['plate', 1]], 'pieces': [['dustbin', 0], ['plate', 0]], 'wet': [['dustbin', 0], ['plate', 0]], 'open': [['dustbin', 0], ['plate', 0]], 'temperature': [['dustbin', 0], ['plate', 0]], 'solid': [['dustbin', 0], ['plate', 0]], 'contain': [['dustbin', 6], ['plate', 0]], 'running': [['dustbin', 0], ['plate', 0]], 'moveable': [['dustbin', 0], ['plate', 2]], 'mixed': [['dustbin', 0], ['plate', 0]], 'edible': [['dustbin', 0], ['plate', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['fridge', 0], ['soup', 2]], 'exist': [['fridge', 2], ['soup', 2]], 'clean': [['fridge', 0], ['soup', 0]], 'power': [['fridge', 0], ['soup', 0]], 'functional': [['fridge', 2], ['soup', 2]], 'pieces': [['fridge', 0], ['soup', 0]], 'wet': [['fridge', 0], ['soup', 0]], 'open': [['fridge', 8], ['soup', 0]], 'temperature': [['fridge', 0], ['soup', 1]], 'solid': [['fridge', 0], ['soup', 0]], 'contain': [['fridge', 8], ['soup', 0]], 'running': [['fridge', 0], ['soup', 0]], 'moveable': [['fridge', 2], ['soup', 2]], 'mixed': [['fridge', 0], ['soup', 0]], 'edible': [['fridge', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0], ['soup', 3]], 'exist': [['microwave', 2], ['soup', 2]], 'clean': [['microwave', 0], ['soup', 0]], 'power': [['microwave', 2], ['soup', 0]], 'functional': [['microwave', 2], ['soup', 2]], 'pieces': [['microwave', 0], ['soup', 0]], 'wet': [['microwave', 0], ['soup', 0]], 'open': [['microwave', 8], ['soup', 0]], 'temperature': [['microwave', 0], ['soup', 0]], 'solid': [['microwave', 0], ['soup', 0]], 'contain': [['microwave', 6], ['soup', 0]], 'running': [['microwave', 0], ['soup', 0]], 'moveable': [['microwave', 2], ['soup', 2]], 'mixed': [['microwave', 0], ['soup', 0]], 'edible': [['microwave', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0]], 'exist': [['microwave', 2]], 'clean': [['microwave', 0]], 'power': [['microwave', 2]], 'functional': [['microwave', 2]], 'pieces': [['microwave', 0]], 'wet': [['microwave', 0]], 'open': [['microwave', 1]], 'temperature': [['microwave', 0]], 'solid': [['microwave', 0]], 'contain': [['microwave', 2]], 'running': [['microwave', 4]], 'moveable': [['microwave', 2]], 'mixed': [['microwave', 0]], 'edible': [['microwave', 0]]}]}\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  label: \n",
      "    1,\n",
      "  breakpoint: \n",
      "    4,\n",
      "  confl_sents: \n",
      "    [\n",
      "      3\n",
      "    ],\n",
      "  confl_pairs: \n",
      "    [\n",
      "      [3, 4]\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from www.utils import print_dict\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "data_file = os.path.join(DRIVE_PATH, 'all_data/www_2s_new.json')\n",
    "with open(data_file, 'r') as f:\n",
    "  cloze_dataset_2s, order_dataset_2s = json.load(f)  \n",
    "\n",
    "for p in cloze_dataset_2s:\n",
    "  label_dist = Counter([ex['label'] for ex in cloze_dataset_2s[p]])\n",
    "  print('Cloze label distribution (%s):' % p)\n",
    "  print(label_dist.most_common())\n",
    "print_dict(cloze_dataset_2s['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxIYaEobhR7J"
   },
   "source": [
    "---\n",
    "\n",
    "# TRIP Results\n",
    "\n",
    "Contains code for the tiered and random TRIP baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mbUgyE0bbJqn"
   },
   "outputs": [],
   "source": [
    "if task_name != 'trip':\n",
    "  raise ValueError('Please configure task_name in first cell to \"trip\" to run TRIP results!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ctQweSlAceo"
   },
   "source": [
    "## Transformer-Based Tiered Classifier for TRIP\n",
    "\n",
    "This is the baseline model presented in the paper. Based on the settings above, the below cells can be used for training and evaluating models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q-xjfYU_cV8"
   },
   "source": [
    "### Featurization for Tiered Classification\n",
    "\n",
    "Get the data ready for input to the model.\n",
    "If you want to use feature augmentation, set Feature_augmentation to be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[========================================================================] 100%\n",
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "from www.dataset.ann import att_to_idx, att_change_dir, att_types, att_default_values\n",
    "from www.dataset.prepro import get_tiered_data, balance_labels\n",
    "from www.dataset.featurize import add_bert_features_tiered, get_tensor_dataset_tiered\n",
    "from collections import Counter\n",
    "import spacy\n",
    "feature_augmentation=False\n",
    "tiered_dataset = cloze_dataset_2s\n",
    "train_spans = False\n",
    "debug_mode=False\n",
    "if(debug_mode):\n",
    "    tiered_dataset['train']=tiered_dataset['train'][:20]\n",
    "    tiered_dataset['dev']=tiered_dataset['dev'][:20]\n",
    "if(not feature_augmentation):\n",
    "    # train_spans = True\n",
    "    train_spans = False\n",
    "    if train_spans:\n",
    "      tiered_dataset = get_story_spans_2s(tiered_dataset, train_only=True)\n",
    "      tiered_dataset['train'] = [ex for ex in tiered_dataset['train'] if ex['label'] != -1] # For now, ignore examples where both stories are plausible :(\n",
    "\n",
    "    seq_length = 16 # Max sequence length to pad to\n",
    "\n",
    "    tiered_dataset = get_tiered_data(tiered_dataset)\n",
    "    tiered_dataset = add_bert_features_tiered(tiered_dataset, tokenizer, seq_length, add_segment_ids=True)\n",
    "\n",
    "    tiered_tensor_dataset = {}\n",
    "    max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n",
    "    for p in tiered_dataset:\n",
    "      tiered_tensor_dataset[p] = get_tensor_dataset_tiered(tiered_dataset[p], max_story_length, add_segment_ids=True)\n",
    "else:\n",
    "    dataset=tiered_dataset\n",
    "  # states = {p: [] for p in dataset}\n",
    "    max_story_length = max([len(ex['sentences']) for p in dataset for ex_2s in dataset[p] for ex in ex_2s['stories']])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for p in dataset:\n",
    "        for ex_2s in dataset[p]:\n",
    "          for s_idx, ex in enumerate(ex_2s['stories']):\n",
    "            if 'states' in ex:\n",
    "              ent_sent_examples = {}\n",
    "              all_entities = set()\n",
    "              for i, sent_ann in enumerate(ex['states']):\n",
    "                #each sentence\n",
    "                entities = []\n",
    "                entity_anns = {}\n",
    "                noun_to_verb={}\n",
    "                doc = nlp(ex['sentences'][i])\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    if(chunk.root.dep_=='dobj'):\n",
    "                        noun_to_verb[chunk.root.text]=chunk.root.head.text\n",
    "                #create a dict which relates verbs and entity\n",
    "                for att in sent_ann:\n",
    "                #for each attribute\n",
    "                  for ent, v in [tuple(ann) for ann in sent_ann[att]]:\n",
    "                    #entity, label\n",
    "                    entities.append(ent)\n",
    "                    ent_verb=noun_to_verb.get(ent)\n",
    "\n",
    "                    if(ent_verb!=None):\n",
    "                        entities.append(ent_verb)\n",
    "                    if ent not in entity_anns:\n",
    "                      entity_anns[ent] = [[0] * len(att_to_idx), [0] * len(att_to_idx)] # pre/post condition, then value for each attribute\n",
    "                    if(ent_verb not in entity_anns):\n",
    "                      entity_anns[ent_verb] = [[0] * len(att_to_idx), [0] * len(att_to_idx)] # pre/post condition, then value for each attribute  \n",
    "                    if 'location' not in att:\n",
    "                      entity_anns[ent][0][att_to_idx[att]] = att_change_dir['default'][v][0] + 1\n",
    "                      entity_anns[ent][1][att_to_idx[att]] = att_change_dir['default'][v][1] + 1\n",
    "                      entity_anns[ent_verb][0][att_to_idx[att]] = att_change_dir['default'][v][0] + 1\n",
    "                      entity_anns[ent_verb][1][att_to_idx[att]] = att_change_dir['default'][v][1] + 1\n",
    "                    else:\n",
    "                      # For location, just use original label space - NOTE: missing this caused an issue for the \"location\" attribute in our original submission, but not the \"h_location\" attribute\n",
    "                      entity_anns[ent][0][att_to_idx[att]] = v\n",
    "                      entity_anns[ent][1][att_to_idx[att]] = v\n",
    "                      entity_anns[ent_verb][0][att_to_idx[att]] = v\n",
    "                      entity_anns[ent_verb][1][att_to_idx[att]] = v\n",
    "\n",
    "\n",
    "                entities = list(set(entities))\n",
    "                all_entities = all_entities.union(set(entities))\n",
    "                for ent in entities:\n",
    "                  states_ex = {}\n",
    "                  states_ex['example_id'] = ex_2s['example_id'] + '-%s-%s-%s' % (str(s_idx), str(i), ent)\n",
    "                  states_ex['base_id'] = ex['example_id']\n",
    "                  states_ex['sentence_idx'] = i\n",
    "                  states_ex['entity'] = ent\n",
    "                  states_ex['sentence'] = ex['sentences'][i]\n",
    "                  states_ex['preconditions'] = entity_anns[ent][0]\n",
    "                  states_ex['effects'] = entity_anns[ent][1]\n",
    "\n",
    "                  ent_sent_examples[(ent, i)] = states_ex\n",
    "\n",
    "            ex_2s['stories'][s_idx]['entities'] = [None for _ in range(len(all_entities))] # entity-story data: preconditions, effects, etc.\n",
    "            for ei, ent in enumerate(all_entities):\n",
    "              ent_ex = {}\n",
    "              ent_ex['example_id'] = ex_2s['example_id'] + '-' + str(s_idx) + '-' + ent\n",
    "              ent_ex['base_id'] = ex_2s['example_id'] + '-' + str(s_idx)\n",
    "              ent_ex['sentences'] = ex['sentences']\n",
    "              ent_ex['entity'] = ent\n",
    "              ent_ex['attributes'] = np.zeros((max_story_length, len(att_to_idx)))\n",
    "              ent_ex['preconditions'] = np.zeros((max_story_length, len(att_to_idx)))\n",
    "              ent_ex['effects'] = np.zeros((max_story_length, len(att_to_idx)))\n",
    "\n",
    "              ent_ex['conflict_span'] = (0,0)\n",
    "              ent_ex['conflict_span_onehot'] = np.zeros((max_story_length))\n",
    "              ent_ex['plausible'] = 1\n",
    "              if s_idx != ex_2s['label'] and ex_2s['label'] != -1:\n",
    "                # print(ex_2s['confl_sents'])\n",
    "                conflict_span = (max([s+1 for s in ex_2s['confl_sents'] if s < ex_2s['breakpoint']]), ex_2s['breakpoint']+1) \n",
    "\n",
    "                # Check if the entity has some nontrivial annotated states in the boundaries of the conflict span\n",
    "                if (ent, conflict_span[0]-1) in ent_sent_examples:\n",
    "                  for i, att in enumerate(att_default_values):\n",
    "                    if (ent_sent_examples[(ent, conflict_span[0]-1)]['preconditions'][i] != att_default_values[att] or ent_sent_examples[(ent, conflict_span[0]-1)]['effects'][i] != att_default_values[att]):\n",
    "                      ent_ex['conflict_span'] = conflict_span\n",
    "                      ent_ex['plausible'] = 0\n",
    "                if (ent, conflict_span[1]-1) in ent_sent_examples:\n",
    "                  for i, att in enumerate(att_default_values):\n",
    "                    if (ent_sent_examples[(ent, conflict_span[1]-1)]['preconditions'][i] != att_default_values[att] or ent_sent_examples[(ent, conflict_span[1]-1)]['effects'][i] != att_default_values[att]):\n",
    "                      ent_ex['conflict_span'] = conflict_span\n",
    "                      ent_ex['plausible'] = 0\n",
    "\n",
    "              for cs in ent_ex['conflict_span']:\n",
    "                if cs > 0:\n",
    "                  ent_ex['conflict_span_onehot'][cs-1] = 1\n",
    "\n",
    "              # Get binary label for each span of text as well (for alternative formulation)\n",
    "              ent_ex['span_labels'] = np.zeros((max_story_length * (max_story_length - 1) // 2))\n",
    "              if s_idx == 1 - ex_2s['label']: # If this is the implausible choice\n",
    "                span_idx = 0\n",
    "                for s2 in range(1, len(ex['sentences'])):\n",
    "                  for s1 in range(s2):\n",
    "                    # print(ex['confl_pairs'])\n",
    "                    for p1, p2 in ex_2s['confl_pairs']:\n",
    "                      if s1 <= p1 and s2 >= p2:\n",
    "                        # Check if the entity has some nontrivial annotated states in the boundaries of the conflict span\n",
    "                        if (ent, p1) in ent_sent_examples:\n",
    "                          for i, att in enumerate(att_default_values):\n",
    "                            if ent_sent_examples[(ent, p1)]['preconditions'][i] != att_default_values[att] or ent_sent_examples[(ent, p1)]['effects'][i] != att_default_values[att]:\n",
    "                              ent_ex['span_labels'][span_idx] = 1\n",
    "                        if (ent, p2) in ent_sent_examples:\n",
    "                          for i, att in enumerate(att_default_values):\n",
    "                            if ent_sent_examples[(ent, p2)]['preconditions'][i] != att_default_values[att] or ent_sent_examples[(ent, p2)]['effects'][i] != att_default_values[att]:\n",
    "                              ent_ex['span_labels'][span_idx] = 1\n",
    "                    span_idx += 1\n",
    "\n",
    "              for i in range(ex_2s['length']):\n",
    "                if (ent, i) in ent_sent_examples:\n",
    "                  ent_ex['preconditions'][i,:] = ent_sent_examples[(ent, i)]['preconditions']\n",
    "                  ent_ex['effects'][i,:] = ent_sent_examples[(ent, i)]['effects']\n",
    "                  for j, att in enumerate(att_default_values):\n",
    "                    if ent_ex['preconditions'][i,j] != att_default_values[att] or ent_ex['effects'][i,j] != att_default_values[att]:\n",
    "                      ent_ex['attributes'][i,j] = 1\n",
    "              ex_2s['stories'][s_idx]['entities'][ei] = ent_ex\n",
    "    tiered_dataset=dataset\n",
    "    seq_length = 16 # Max sequence length to pad to\n",
    "    tiered_dataset = add_bert_features_tiered(tiered_dataset, tokenizer, seq_length, add_segment_ids=True)\n",
    "    tiered_tensor_dataset = {}\n",
    "    max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n",
    "    for p in tiered_dataset:\n",
    "      tiered_tensor_dataset[p] = get_tensor_dataset_tiered(tiered_dataset[p], max_story_length, add_segment_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fQ6wXQIBdq1"
   },
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-BMInyrBdq2"
   },
   "source": [
    "#### Configure Hyperparameters\n",
    "We will perform grid search over (batch size, learning rate). Configure the training sub-task, search space and set the maximum number of training epochs here. Currently configured for re-training the best RoBERTa-based model instance. Read code comments for more information.\n",
    "\n",
    "**Additional configuration options:**\n",
    "* Change the `generate_learning_curve` variable to `True` to generate data for training curves in the style presented in the paper.\n",
    "* You may ablate the input to the Conflict Detector based on a few pre-defined ablation modes. To do so, change the `ablation` variable based on the comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tvfTuEYRBdq3"
   },
   "outputs": [],
   "source": [
    "from www.dataset.ann import att_to_idx, att_to_num_classes, att_types\n",
    "\n",
    "subtask = 'cloze'\n",
    "batch_sizes = [config_batch_size]\n",
    "learning_rates = [config_lr]\n",
    "epochs = config_epochs\n",
    "eval_batch_size = 16\n",
    "generate_learning_curve = True # Generate data for training curve figure in TRIP paper\n",
    "\n",
    "num_state_labels = {}\n",
    "for att in att_to_idx:\n",
    "  if att_types[att] == 'default':\n",
    "    num_state_labels[att_to_idx[att]] = 3\n",
    "  else:\n",
    "    num_state_labels[att_to_idx[att]] = att_to_num_classes[att] # Location attributes fall into this since they don't have well-define pre- and post-condition yet\n",
    "\n",
    "# Ablation options:\n",
    "# - attributes: skip attribute prediction phase\n",
    "# - embeddings: DON'T input contextual embeddings to conflict detector\n",
    "# - states: DON'T input states to conflict detector\n",
    "# - states-labels: in states input to conflict detector, include predicted labels\n",
    "# - states-logits: in states input to conflict detector, include state logits (preferred)\n",
    "# - states-teacher-forcing: train conflict detector on ground truth state labels (not predictions)\n",
    "# - states-attention: re-weight input to conflict detector with weights conditioned on states representation\n",
    "ablation = ['attributes', 'states-logits'] # This is the default mode presented in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fRC3cnLBdq3"
   },
   "source": [
    "#### Perform Grid Search\n",
    "\n",
    "Perform hyperparameter tuning to find the best story classification model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n"
     ]
    }
   ],
   "source": [
    "print(len(tiered_dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59885,
     "status": "ok",
     "timestamp": 1631299694590,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "DWnCen7NBdq3",
    "outputId": "e6a79d9b-b478-4ea5-e4ef-d50d2a22985d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning grid search for the cloze sub-task over 1 parameter combination(s)!\n",
      "\n",
      "TRAINING MODEL: bs=1, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[0] Validation results:\n",
      "[0] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9931437444543034,\n",
      "  f1: \n",
      "    0.22141924956196202,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9996273291925466,\n",
      "  f1_1: \n",
      "    0.6644556163617592,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9749068322981367,\n",
      "  f1_5: \n",
      "    0.13124384434240646,\n",
      "  accuracy_6: \n",
      "    0.9841348713398402,\n",
      "  f1_6: \n",
      "    0.6284467292912538,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9921472937000887,\n",
      "  f1_8: \n",
      "    0.3320193899442502,\n",
      "  accuracy_9: \n",
      "    0.9857675244010647,\n",
      "  f1_9: \n",
      "    0.6321814703817625,\n",
      "  accuracy_10: \n",
      "    0.9941082519964508,\n",
      "  f1_10: \n",
      "    0.33234847406141727,\n",
      "  accuracy_11: \n",
      "    0.9969476486246672,\n",
      "  f1_11: \n",
      "    0.33282383051329445,\n",
      "  accuracy_12: \n",
      "    0.9926264418811003,\n",
      "  f1_12: \n",
      "    0.3320998594344426,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.993664596273292,\n",
      "  f1_14: \n",
      "    0.33227407730492037,\n",
      "  accuracy_15: \n",
      "    0.9941348713398402,\n",
      "  f1_15: \n",
      "    0.33235293681411177,\n",
      "  accuracy_16: \n",
      "    0.9927772848269742,\n",
      "  f1_16: \n",
      "    0.3321251844133947,\n",
      "  accuracy_17: \n",
      "    0.9768766637089619,\n",
      "  f1_17: \n",
      "    0.5823661803174192,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.996938775510204,\n",
      "  f1_19: \n",
      "    0.3328223471299608,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9927138420585625,\n",
      "  f1: \n",
      "    0.25178561169379987,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9995829636202307,\n",
      "  f1_1: \n",
      "    0.6641974306476072,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9782608695652174,\n",
      "  f1_5: \n",
      "    0.16565646374705104,\n",
      "  accuracy_6: \n",
      "    0.9832741792369122,\n",
      "  f1_6: \n",
      "    0.6261153836795293,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9920940550133097,\n",
      "  f1_8: \n",
      "    0.33201044650028877,\n",
      "  accuracy_9: \n",
      "    0.9833540372670807,\n",
      "  f1_9: \n",
      "    0.6259142441410707,\n",
      "  accuracy_10: \n",
      "    0.9939662821650399,\n",
      "  f1_10: \n",
      "    0.33232467070131716,\n",
      "  accuracy_11: \n",
      "    0.9959006211180125,\n",
      "  f1_11: \n",
      "    0.332648700234435,\n",
      "  accuracy_12: \n",
      "    0.9952972493345165,\n",
      "  f1_12: \n",
      "    0.332547694223329,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9933629103815439,\n",
      "  f1_14: \n",
      "    0.33222346859438895,\n",
      "  accuracy_15: \n",
      "    0.990248447204969,\n",
      "  f1_15: \n",
      "    0.3317001113087622,\n",
      "  accuracy_16: \n",
      "    0.9928039041703638,\n",
      "  f1_16: \n",
      "    0.3321296531293459,\n",
      "  accuracy_17: \n",
      "    0.9761401952085181,\n",
      "  f1_17: \n",
      "    0.5782425587441603,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9692901508429459,\n",
      "  f1: \n",
      "    0.4930679030994201,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.4968944099378882,\n",
      "  f1: \n",
      "    0.4968944099378882,\n",
      "  verifiability: \n",
      "    0.0,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Finished epoch.\n",
      "[1] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[1] Validation results:\n",
      "[1] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9939232475598935,\n",
      "  f1: \n",
      "    0.269721190873293,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9996273291925466,\n",
      "  f1_1: \n",
      "    0.6644562779436248,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.980452528837622,\n",
      "  f1_5: \n",
      "    0.22924285727881497,\n",
      "  accuracy_6: \n",
      "    0.9867613132209405,\n",
      "  f1_6: \n",
      "    0.6339576851037935,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9944454303460515,\n",
      "  f1_8: \n",
      "    0.49386101646031805,\n",
      "  accuracy_9: \n",
      "    0.9886069210292813,\n",
      "  f1_9: \n",
      "    0.638113791762034,\n",
      "  accuracy_10: \n",
      "    0.9941082519964508,\n",
      "  f1_10: \n",
      "    0.33234847406141727,\n",
      "  accuracy_11: \n",
      "    0.9969476486246672,\n",
      "  f1_11: \n",
      "    0.33282383051329445,\n",
      "  accuracy_12: \n",
      "    0.9935758651286601,\n",
      "  f1_12: \n",
      "    0.5004940620174655,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.993664596273292,\n",
      "  f1_14: \n",
      "    0.33227407730492037,\n",
      "  accuracy_15: \n",
      "    0.9941348713398402,\n",
      "  f1_15: \n",
      "    0.33235293681411177,\n",
      "  accuracy_16: \n",
      "    0.9933984028393966,\n",
      "  f1_16: \n",
      "    0.4764480131909907,\n",
      "  accuracy_17: \n",
      "    0.9775865128660159,\n",
      "  f1_17: \n",
      "    0.5916209274440907,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.996938775510204,\n",
      "  f1_19: \n",
      "    0.3328223471299608,\n",
      "}\n",
      "\n",
      "\n",
      "[1] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9933385093167701,\n",
      "  f1: \n",
      "    0.2581074807460084,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9995740905057675,\n",
      "  f1_1: \n",
      "    0.6641457379149839,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9789086069210293,\n",
      "  f1_5: \n",
      "    0.22172978553451805,\n",
      "  accuracy_6: \n",
      "    0.9858828748890861,\n",
      "  f1_6: \n",
      "    0.6314732955658335,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9938952972493346,\n",
      "  f1_8: \n",
      "    0.47005812201799185,\n",
      "  accuracy_9: \n",
      "    0.9860337178349601,\n",
      "  f1_9: \n",
      "    0.6314598303371781,\n",
      "  accuracy_10: \n",
      "    0.9939662821650399,\n",
      "  f1_10: \n",
      "    0.33232467070131716,\n",
      "  accuracy_11: \n",
      "    0.9959006211180125,\n",
      "  f1_11: \n",
      "    0.332648700234435,\n",
      "  accuracy_12: \n",
      "    0.9953327417923691,\n",
      "  f1_12: \n",
      "    0.3389793125165806,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9933629103815439,\n",
      "  f1_14: \n",
      "    0.33222346859438895,\n",
      "  accuracy_15: \n",
      "    0.9912511091393079,\n",
      "  f1_15: \n",
      "    0.3987114308037841,\n",
      "  accuracy_16: \n",
      "    0.9950576752440107,\n",
      "  f1_16: \n",
      "    0.5075598379571679,\n",
      "  accuracy_17: \n",
      "    0.9776131322094055,\n",
      "  f1_17: \n",
      "    0.590190181742369,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[1] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9693788819875776,\n",
      "  f1: \n",
      "    0.493670010368116,\n",
      "}\n",
      "\n",
      "\n",
      "[1] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.5279503105590062,\n",
      "  f1: \n",
      "    0.5277863538129053,\n",
      "  verifiability: \n",
      "    0.0,\n",
      "}\n",
      "\n",
      "\n",
      "[1] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Finished epoch.\n",
      "[2] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[2] Validation results:\n",
      "[2] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9944236912156167,\n",
      "  f1: \n",
      "    0.30264904960764183,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.999600709849157,\n",
      "  f1_1: \n",
      "    0.6642983121964052,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.981561668145519,\n",
      "  f1_5: \n",
      "    0.2349934915106866,\n",
      "  accuracy_6: \n",
      "    0.9857763975155279,\n",
      "  f1_6: \n",
      "    0.6314756389982349,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9959893522626442,\n",
      "  f1_8: \n",
      "    0.5620784551812226,\n",
      "  accuracy_9: \n",
      "    0.9878527062999113,\n",
      "  f1_9: \n",
      "    0.6359564928247222,\n",
      "  accuracy_10: \n",
      "    0.9960337178349601,\n",
      "  f1_10: \n",
      "    0.524785832706712,\n",
      "  accuracy_11: \n",
      "    0.9969476486246672,\n",
      "  f1_11: \n",
      "    0.33282383051329445,\n",
      "  accuracy_12: \n",
      "    0.9941526175687666,\n",
      "  f1_12: \n",
      "    0.5401869230754177,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.9949866903283052,\n",
      "  f1_14: \n",
      "    0.5023520247401215,\n",
      "  accuracy_15: \n",
      "    0.9941348713398402,\n",
      "  f1_15: \n",
      "    0.33235293681411177,\n",
      "  accuracy_16: \n",
      "    0.9955013309671695,\n",
      "  f1_16: \n",
      "    0.7044112474866936,\n",
      "  accuracy_17: \n",
      "    0.9807808340727595,\n",
      "  f1_17: \n",
      "    0.5956827708940284,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.996938775510204,\n",
      "  f1_19: \n",
      "    0.3328223471299608,\n",
      "}\n",
      "\n",
      "\n",
      "[2] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9939689440993789,\n",
      "  f1: \n",
      "    0.26981493715035315,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9995563442768411,\n",
      "  f1_1: \n",
      "    0.6640401281565148,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9815350488021295,\n",
      "  f1_5: \n",
      "    0.23435775697244166,\n",
      "  accuracy_6: \n",
      "    0.984871339840284,\n",
      "  f1_6: \n",
      "    0.6287230544740603,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9960958296362024,\n",
      "  f1_8: \n",
      "    0.5701342614810695,\n",
      "  accuracy_9: \n",
      "    0.9851818988464951,\n",
      "  f1_9: \n",
      "    0.6289417430747611,\n",
      "  accuracy_10: \n",
      "    0.9953682342502218,\n",
      "  f1_10: \n",
      "    0.4897681471905991,\n",
      "  accuracy_11: \n",
      "    0.9959006211180125,\n",
      "  f1_11: \n",
      "    0.332648700234435,\n",
      "  accuracy_12: \n",
      "    0.9968234250221828,\n",
      "  f1_12: \n",
      "    0.5410700450229661,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9945785270629991,\n",
      "  f1_14: \n",
      "    0.5004303114734571,\n",
      "  accuracy_15: \n",
      "    0.9930434782608696,\n",
      "  f1_15: \n",
      "    0.4932898981325315,\n",
      "  accuracy_16: \n",
      "    0.9961401952085182,\n",
      "  f1_16: \n",
      "    0.5672462393235773,\n",
      "  accuracy_17: \n",
      "    0.9802928127772849,\n",
      "  f1_17: \n",
      "    0.5931767685072318,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[2] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9699378881987578,\n",
      "  f1: \n",
      "    0.5736619790129031,\n",
      "}\n",
      "\n",
      "\n",
      "[2] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.6770186335403726,\n",
      "  f1: \n",
      "    0.6770186335403726,\n",
      "  verifiability: \n",
      "    0.049689440993788817,\n",
      "}\n",
      "\n",
      "\n",
      "[2] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Finished epoch.\n",
      "[3] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[3] Validation results:\n",
      "[3] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9946211180124224,\n",
      "  f1: \n",
      "    0.35726352496637964,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9996805678793257,\n",
      "  f1_1: \n",
      "    0.6647706475353331,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9819875776397515,\n",
      "  f1_5: \n",
      "    0.2758555677546053,\n",
      "  accuracy_6: \n",
      "    0.9862555456965395,\n",
      "  f1_6: \n",
      "    0.6336836367646085,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9969032830523514,\n",
      "  f1_8: \n",
      "    0.5953346979884918,\n",
      "  accuracy_9: \n",
      "    0.9878349600709849,\n",
      "  f1_9: \n",
      "    0.6368114400919253,\n",
      "  accuracy_10: \n",
      "    0.9962821650399291,\n",
      "  f1_10: \n",
      "    0.5472702227393053,\n",
      "  accuracy_11: \n",
      "    0.9969476486246672,\n",
      "  f1_11: \n",
      "    0.33282383051329445,\n",
      "  accuracy_12: \n",
      "    0.9963442768411712,\n",
      "  f1_12: \n",
      "    0.7964235477973056,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.9949689440993789,\n",
      "  f1_14: \n",
      "    0.5284728782301529,\n",
      "  accuracy_15: \n",
      "    0.9941348713398402,\n",
      "  f1_15: \n",
      "    0.33235293681411177,\n",
      "  accuracy_16: \n",
      "    0.99622892635315,\n",
      "  f1_16: \n",
      "    0.7855889295635768,\n",
      "  accuracy_17: \n",
      "    0.9790328305235138,\n",
      "  f1_17: \n",
      "    0.593240070416385,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.9976042590949423,\n",
      "  f1_19: \n",
      "    0.49324607835384954,\n",
      "}\n",
      "\n",
      "\n",
      "[3] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9940150842945874,\n",
      "  f1: \n",
      "    0.2944121381783107,\n",
      "  accuracy_0: \n",
      "    0.9951020408163266,\n",
      "  f1_0: \n",
      "    0.33251500272776874,\n",
      "  accuracy_1: \n",
      "    0.9996362023070098,\n",
      "  f1_1: \n",
      "    0.6645124587940191,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9811623779946761,\n",
      "  f1_5: \n",
      "    0.25534029355426685,\n",
      "  accuracy_6: \n",
      "    0.9858118899733807,\n",
      "  f1_6: \n",
      "    0.6811897767894193,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9965661047027506,\n",
      "  f1_8: \n",
      "    0.5909392416528846,\n",
      "  accuracy_9: \n",
      "    0.9853327417923691,\n",
      "  f1_9: \n",
      "    0.6303292361603832,\n",
      "  accuracy_10: \n",
      "    0.9959716060337178,\n",
      "  f1_10: \n",
      "    0.5464505071250509,\n",
      "  accuracy_11: \n",
      "    0.9959006211180125,\n",
      "  f1_11: \n",
      "    0.332648700234435,\n",
      "  accuracy_12: \n",
      "    0.9968411712511092,\n",
      "  f1_12: \n",
      "    0.5407518706592461,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9947382431233363,\n",
      "  f1_14: \n",
      "    0.5362795909148028,\n",
      "  accuracy_15: \n",
      "    0.9934250221827862,\n",
      "  f1_15: \n",
      "    0.514550441646485,\n",
      "  accuracy_16: \n",
      "    0.9963087843833185,\n",
      "  f1_16: \n",
      "    0.57952217857377,\n",
      "  accuracy_17: \n",
      "    0.9786157941437444,\n",
      "  f1_17: \n",
      "    0.5909990136625091,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[3] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9696273291925466,\n",
      "  f1: \n",
      "    0.6723754292611603,\n",
      "}\n",
      "\n",
      "\n",
      "[3] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7329192546583851,\n",
      "  f1: \n",
      "    0.732919254658385,\n",
      "  verifiability: \n",
      "    0.08074534161490683,\n",
      "}\n",
      "\n",
      "\n",
      "[3] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Finished epoch.\n",
      "[4] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[4] Validation results:\n",
      "[4] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9949272404614019,\n",
      "  f1: \n",
      "    0.3591101140708064,\n",
      "  accuracy_0: \n",
      "    0.996681455190772,\n",
      "  f1_0: \n",
      "    0.49623559795341304,\n",
      "  accuracy_1: \n",
      "    0.9996184560780834,\n",
      "  f1_1: \n",
      "    0.664403843120053,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9827506654835847,\n",
      "  f1_5: \n",
      "    0.2791196840685756,\n",
      "  accuracy_6: \n",
      "    0.9870807453416149,\n",
      "  f1_6: \n",
      "    0.6353171532800164,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9966903283052352,\n",
      "  f1_8: \n",
      "    0.5892119148784073,\n",
      "  accuracy_9: \n",
      "    0.9888464951197871,\n",
      "  f1_9: \n",
      "    0.6392451032500026,\n",
      "  accuracy_10: \n",
      "    0.9961756876663709,\n",
      "  f1_10: \n",
      "    0.5424600964380273,\n",
      "  accuracy_11: \n",
      "    0.9969476486246672,\n",
      "  f1_11: \n",
      "    0.33282383051329445,\n",
      "  accuracy_12: \n",
      "    0.9962023070097604,\n",
      "  f1_12: \n",
      "    0.7975914378681473,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.9953593611357586,\n",
      "  f1_14: \n",
      "    0.5488811250030118,\n",
      "  accuracy_15: \n",
      "    0.9943123336291038,\n",
      "  f1_15: \n",
      "    0.35273875033549107,\n",
      "  accuracy_16: \n",
      "    0.9960425909494233,\n",
      "  f1_16: \n",
      "    0.7668804267535302,\n",
      "  accuracy_17: \n",
      "    0.98075421472937,\n",
      "  f1_17: \n",
      "    0.5954419906718381,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.9979680567879325,\n",
      "  f1_19: \n",
      "    0.550681020690961,\n",
      "}\n",
      "\n",
      "\n",
      "[4] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9943571428571428,\n",
      "  f1: \n",
      "    0.30784381853558646,\n",
      "  accuracy_0: \n",
      "    0.9961845607808341,\n",
      "  f1_0: \n",
      "    0.45463455006563985,\n",
      "  accuracy_1: \n",
      "    0.9995740905057675,\n",
      "  f1_1: \n",
      "    0.6641457379149839,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9823336291038154,\n",
      "  f1_5: \n",
      "    0.2642948689149396,\n",
      "  accuracy_6: \n",
      "    0.9870807453416149,\n",
      "  f1_6: \n",
      "    0.7517472834669884,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9966193433895297,\n",
      "  f1_8: \n",
      "    0.5924015921918436,\n",
      "  accuracy_9: \n",
      "    0.9862910381543922,\n",
      "  f1_9: \n",
      "    0.6327009906897445,\n",
      "  accuracy_10: \n",
      "    0.9959627329192546,\n",
      "  f1_10: \n",
      "    0.5683485928608245,\n",
      "  accuracy_11: \n",
      "    0.9959006211180125,\n",
      "  f1_11: \n",
      "    0.332648700234435,\n",
      "  accuracy_12: \n",
      "    0.9973291925465838,\n",
      "  f1_12: \n",
      "    0.578486138896558,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9948890860692103,\n",
      "  f1_14: \n",
      "    0.5391381336461207,\n",
      "  accuracy_15: \n",
      "    0.9935048802129548,\n",
      "  f1_15: \n",
      "    0.5422097693074436,\n",
      "  accuracy_16: \n",
      "    0.9964951197870453,\n",
      "  f1_16: \n",
      "    0.5847902388570317,\n",
      "  accuracy_17: \n",
      "    0.9800887311446318,\n",
      "  f1_17: \n",
      "    0.5922187444657862,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[4] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.968890860692103,\n",
      "  f1: \n",
      "    0.6821956687861515,\n",
      "}\n",
      "\n",
      "\n",
      "[4] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7267080745341615,\n",
      "  f1: \n",
      "    0.7266131522074714,\n",
      "  verifiability: \n",
      "    0.09006211180124224,\n",
      "}\n",
      "\n",
      "\n",
      "[4] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Finished epoch.\n",
      "[5] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[5] Validation results:\n",
      "[5] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9949170363797692,\n",
      "  f1: \n",
      "    0.466601179096695,\n",
      "  accuracy_0: \n",
      "    0.9975510204081632,\n",
      "  f1_0: \n",
      "    0.5588133876798828,\n",
      "  accuracy_1: \n",
      "    0.9996184560780834,\n",
      "  f1_1: \n",
      "    0.6644031659960009,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9831854480922804,\n",
      "  f1_5: \n",
      "    0.38423820065083475,\n",
      "  accuracy_6: \n",
      "    0.9862910381543922,\n",
      "  f1_6: \n",
      "    0.6333914990000288,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9965306122448979,\n",
      "  f1_8: \n",
      "    0.584674989850725,\n",
      "  accuracy_9: \n",
      "    0.9882342502218279,\n",
      "  f1_9: \n",
      "    0.637871561737185,\n",
      "  accuracy_10: \n",
      "    0.996335403726708,\n",
      "  f1_10: \n",
      "    0.5555949992731832,\n",
      "  accuracy_11: \n",
      "    0.9969476486246672,\n",
      "  f1_11: \n",
      "    0.33282383051329445,\n",
      "  accuracy_12: \n",
      "    0.9964418811002662,\n",
      "  f1_12: \n",
      "    0.8216458793119923,\n",
      "  accuracy_13: \n",
      "    0.9978083407275954,\n",
      "  f1_13: \n",
      "    0.35828409227357705,\n",
      "  accuracy_14: \n",
      "    0.9951730257320319,\n",
      "  f1_14: \n",
      "    0.5460425573879616,\n",
      "  accuracy_15: \n",
      "    0.994374445430346,\n",
      "  f1_15: \n",
      "    0.3614218825693272,\n",
      "  accuracy_16: \n",
      "    0.9959006211180125,\n",
      "  f1_16: \n",
      "    0.7740005260504202,\n",
      "  accuracy_17: \n",
      "    0.9806921029281278,\n",
      "  f1_17: \n",
      "    0.5978601357602439,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.9978970718722272,\n",
      "  f1_19: \n",
      "    0.5419631006270257,\n",
      "}\n",
      "\n",
      "\n",
      "[5] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9943469387755102,\n",
      "  f1: \n",
      "    0.4065121679553034,\n",
      "  accuracy_0: \n",
      "    0.9975155279503105,\n",
      "  f1_0: \n",
      "    0.5548802576799702,\n",
      "  accuracy_1: \n",
      "    0.9995740905057675,\n",
      "  f1_1: \n",
      "    0.6641449808280595,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9823247559893523,\n",
      "  f1_5: \n",
      "    0.3646888323181263,\n",
      "  accuracy_6: \n",
      "    0.9862732919254659,\n",
      "  f1_6: \n",
      "    0.7359055978424084,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9964063886424135,\n",
      "  f1_8: \n",
      "    0.5842693916888672,\n",
      "  accuracy_9: \n",
      "    0.9856344276841171,\n",
      "  f1_9: \n",
      "    0.6312213583509788,\n",
      "  accuracy_10: \n",
      "    0.9963886424134871,\n",
      "  f1_10: \n",
      "    0.6636198821755065,\n",
      "  accuracy_11: \n",
      "    0.9959006211180125,\n",
      "  f1_11: \n",
      "    0.332648700234435,\n",
      "  accuracy_12: \n",
      "    0.9970896184560781,\n",
      "  f1_12: \n",
      "    0.564084154598634,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9948447204968944,\n",
      "  f1_14: \n",
      "    0.5483330338385768,\n",
      "  accuracy_15: \n",
      "    0.9933451641526175,\n",
      "  f1_15: \n",
      "    0.5400875647338069,\n",
      "  accuracy_16: \n",
      "    0.9963797692990239,\n",
      "  f1_16: \n",
      "    0.5856831139129631,\n",
      "  accuracy_17: \n",
      "    0.9803726708074534,\n",
      "  f1_17: \n",
      "    0.5962344813616568,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[5] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9660159716060337,\n",
      "  f1: \n",
      "    0.6936167994775093,\n",
      "}\n",
      "\n",
      "\n",
      "[5] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7515527950310559,\n",
      "  f1: \n",
      "    0.7515432098765433,\n",
      "  verifiability: \n",
      "    0.07763975155279502,\n",
      "}\n",
      "\n",
      "\n",
      "[5] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Finished epoch.\n",
      "[6] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[6] Validation results:\n",
      "[6] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9950483584738243,\n",
      "  f1: \n",
      "    0.4506916185008121,\n",
      "  accuracy_0: \n",
      "    0.9973114463176574,\n",
      "  f1_0: \n",
      "    0.5431911914005032,\n",
      "  accuracy_1: \n",
      "    0.999600709849157,\n",
      "  f1_1: \n",
      "    0.6642990203760476,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9827861579414374,\n",
      "  f1_5: \n",
      "    0.36397872497028927,\n",
      "  accuracy_6: \n",
      "    0.9872315882874889,\n",
      "  f1_6: \n",
      "    0.6355161812723328,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9966370896184561,\n",
      "  f1_8: \n",
      "    0.5874438529868354,\n",
      "  accuracy_9: \n",
      "    0.9892280390417036,\n",
      "  f1_9: \n",
      "    0.6399503623570415,\n",
      "  accuracy_10: \n",
      "    0.9961756876663709,\n",
      "  f1_10: \n",
      "    0.5402254581406759,\n",
      "  accuracy_11: \n",
      "    0.9975066548358473,\n",
      "  f1_11: \n",
      "    0.47283470105441344,\n",
      "  accuracy_12: \n",
      "    0.9963442768411712,\n",
      "  f1_12: \n",
      "    0.8141620534497461,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.9952351375332742,\n",
      "  f1_14: \n",
      "    0.5426364580534757,\n",
      "  accuracy_15: \n",
      "    0.9944276841171251,\n",
      "  f1_15: \n",
      "    0.36533587742496726,\n",
      "  accuracy_16: \n",
      "    0.9962732919254659,\n",
      "  f1_16: \n",
      "    0.7987285701587066,\n",
      "  accuracy_17: \n",
      "    0.9812422360248447,\n",
      "  f1_17: \n",
      "    0.5993136932447748,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.9978527062999113,\n",
      "  f1_19: \n",
      "    0.5379703806640102,\n",
      "}\n",
      "\n",
      "\n",
      "[6] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9944751552795031,\n",
      "  f1: \n",
      "    0.40318181349893145,\n",
      "  accuracy_0: \n",
      "    0.9970984915705413,\n",
      "  f1_0: \n",
      "    0.5276532706181496,\n",
      "  accuracy_1: \n",
      "    0.9995563442768411,\n",
      "  f1_1: \n",
      "    0.6640409162979889,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9824046140195208,\n",
      "  f1_5: \n",
      "    0.357676868345399,\n",
      "  accuracy_6: \n",
      "    0.9871694764862466,\n",
      "  f1_6: \n",
      "    0.7518428280524576,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9966637089618456,\n",
      "  f1_8: \n",
      "    0.5887319766259449,\n",
      "  accuracy_9: \n",
      "    0.9865749778172138,\n",
      "  f1_9: \n",
      "    0.6330684639867378,\n",
      "  accuracy_10: \n",
      "    0.9962200532386868,\n",
      "  f1_10: \n",
      "    0.6225311078324683,\n",
      "  accuracy_11: \n",
      "    0.99622892635315,\n",
      "  f1_11: \n",
      "    0.4154920919669958,\n",
      "  accuracy_12: \n",
      "    0.9975776397515528,\n",
      "  f1_12: \n",
      "    0.57748782171024,\n",
      "  accuracy_13: \n",
      "    0.9957409050576752,\n",
      "  f1_13: \n",
      "    0.33873815826689474,\n",
      "  accuracy_14: \n",
      "    0.9949334516415261,\n",
      "  f1_14: \n",
      "    0.545460359384312,\n",
      "  accuracy_15: \n",
      "    0.9930967169476487,\n",
      "  f1_15: \n",
      "    0.517552905129211,\n",
      "  accuracy_16: \n",
      "    0.9961845607808341,\n",
      "  f1_16: \n",
      "    0.5730874985226495,\n",
      "  accuracy_17: \n",
      "    0.980851818988465,\n",
      "  f1_17: \n",
      "    0.5973913341413576,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977373558118899,\n",
      "  f1_19: \n",
      "    0.34300604005046337,\n",
      "}\n",
      "\n",
      "\n",
      "[6] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9706122448979592,\n",
      "  f1: \n",
      "    0.6619471253056299,\n",
      "}\n",
      "\n",
      "\n",
      "[6] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7484472049689441,\n",
      "  f1: \n",
      "    0.7484447787969097,\n",
      "  verifiability: \n",
      "    0.10559006211180125,\n",
      "}\n",
      "\n",
      "\n",
      "[6] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Finished epoch.\n",
      "[7] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[7] Validation results:\n",
      "[7] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9949188110026619,\n",
      "  f1: \n",
      "    0.5078437440551713,\n",
      "  accuracy_0: \n",
      "    0.9978260869565218,\n",
      "  f1_0: \n",
      "    0.5749156390418649,\n",
      "  accuracy_1: \n",
      "    0.9996184560780834,\n",
      "  f1_1: \n",
      "    0.664403843120053,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9822537710736469,\n",
      "  f1_5: \n",
      "    0.41791718678432094,\n",
      "  accuracy_6: \n",
      "    0.9865572315882875,\n",
      "  f1_6: \n",
      "    0.6337198671566654,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9963265306122449,\n",
      "  f1_8: \n",
      "    0.5794289993850253,\n",
      "  accuracy_9: \n",
      "    0.9885093167701864,\n",
      "  f1_9: \n",
      "    0.638036186236956,\n",
      "  accuracy_10: \n",
      "    0.9963708961845608,\n",
      "  f1_10: \n",
      "    0.5581356425474282,\n",
      "  accuracy_11: \n",
      "    0.9977018633540373,\n",
      "  f1_11: \n",
      "    0.504489530946746,\n",
      "  accuracy_12: \n",
      "    0.9958740017746229,\n",
      "  f1_12: \n",
      "    0.7854272437305833,\n",
      "  accuracy_13: \n",
      "    0.9977551020408163,\n",
      "  f1_13: \n",
      "    0.33295876323764767,\n",
      "  accuracy_14: \n",
      "    0.9951020408163266,\n",
      "  f1_14: \n",
      "    0.5435221631005511,\n",
      "  accuracy_15: \n",
      "    0.9948092280390417,\n",
      "  f1_15: \n",
      "    0.4073613058181665,\n",
      "  accuracy_16: \n",
      "    0.9961490683229813,\n",
      "  f1_16: \n",
      "    0.7673421874529023,\n",
      "  accuracy_17: \n",
      "    0.9803726708074534,\n",
      "  f1_17: \n",
      "    0.594782216082132,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.997790594498669,\n",
      "  f1_19: \n",
      "    0.5375847473899603,\n",
      "}\n",
      "\n",
      "\n",
      "[7] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9943194321206743,\n",
      "  f1: \n",
      "    0.4201270068493283,\n",
      "  accuracy_0: \n",
      "    0.9976574977817214,\n",
      "  f1_0: \n",
      "    0.5655040019842045,\n",
      "  accuracy_1: \n",
      "    0.9995740905057675,\n",
      "  f1_1: \n",
      "    0.6641457379149839,\n",
      "  accuracy_2: \n",
      "    0.9986335403726708,\n",
      "  f1_2: \n",
      "    0.3331054343547351,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.981606033717835,\n",
      "  f1_5: \n",
      "    0.3724528713727596,\n",
      "  accuracy_6: \n",
      "    0.9864152617568767,\n",
      "  f1_6: \n",
      "    0.7476796730175289,\n",
      "  accuracy_7: \n",
      "    0.9957497781721384,\n",
      "  f1_7: \n",
      "    0.3326234544573428,\n",
      "  accuracy_8: \n",
      "    0.9962644188110027,\n",
      "  f1_8: \n",
      "    0.5781775659268337,\n",
      "  accuracy_9: \n",
      "    0.9859183673469388,\n",
      "  f1_9: \n",
      "    0.6754040485646385,\n",
      "  accuracy_10: \n",
      "    0.9962821650399291,\n",
      "  f1_10: \n",
      "    0.6977807783651858,\n",
      "  accuracy_11: \n",
      "    0.9962821650399291,\n",
      "  f1_11: \n",
      "    0.4282536930063246,\n",
      "  accuracy_12: \n",
      "    0.9973203194321206,\n",
      "  f1_12: \n",
      "    0.5756634613648353,\n",
      "  accuracy_13: \n",
      "    0.9957142857142857,\n",
      "  f1_13: \n",
      "    0.33261751371987597,\n",
      "  accuracy_14: \n",
      "    0.9947559893522626,\n",
      "  f1_14: \n",
      "    0.5425125769521771,\n",
      "  accuracy_15: \n",
      "    0.9933096716947649,\n",
      "  f1_15: \n",
      "    0.519594163809592,\n",
      "  accuracy_16: \n",
      "    0.9962821650399291,\n",
      "  f1_16: \n",
      "    0.5760538475989062,\n",
      "  accuracy_17: \n",
      "    0.9798314108251996,\n",
      "  f1_17: \n",
      "    0.5923115823886956,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[7] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9680124223602484,\n",
      "  f1: \n",
      "    0.6983556450465037,\n",
      "}\n",
      "\n",
      "\n",
      "[7] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.782608695652174,\n",
      "  f1: \n",
      "    0.7826003086419753,\n",
      "  verifiability: \n",
      "    0.06832298136645963,\n",
      "}\n",
      "\n",
      "\n",
      "[7] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] Finished epoch.\n",
      "[8] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[8] Validation results:\n",
      "[8] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9951712511091393,\n",
      "  f1: \n",
      "    0.5275966033098849,\n",
      "  accuracy_0: \n",
      "    0.997941437444543,\n",
      "  f1_0: \n",
      "    0.5826323524948002,\n",
      "  accuracy_1: \n",
      "    0.9995563442768411,\n",
      "  f1_1: \n",
      "    0.6640372367812474,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9833362910381543,\n",
      "  f1_5: \n",
      "    0.43618472215106124,\n",
      "  accuracy_6: \n",
      "    0.9869476486246672,\n",
      "  f1_6: \n",
      "    0.6347100409378332,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9966992014196984,\n",
      "  f1_8: \n",
      "    0.5908403040918948,\n",
      "  accuracy_9: \n",
      "    0.9887843833185448,\n",
      "  f1_9: \n",
      "    0.638659567745795,\n",
      "  accuracy_10: \n",
      "    0.9963708961845608,\n",
      "  f1_10: \n",
      "    0.5583823366925348,\n",
      "  accuracy_11: \n",
      "    0.9977107364685004,\n",
      "  f1_11: \n",
      "    0.522699005889694,\n",
      "  accuracy_12: \n",
      "    0.9965306122448979,\n",
      "  f1_12: \n",
      "    0.8311092084642594,\n",
      "  accuracy_13: \n",
      "    0.9983052351375332,\n",
      "  f1_13: \n",
      "    0.5261968946726044,\n",
      "  accuracy_14: \n",
      "    0.9951996450754215,\n",
      "  f1_14: \n",
      "    0.5489500379865205,\n",
      "  accuracy_15: \n",
      "    0.9952351375332742,\n",
      "  f1_15: \n",
      "    0.44608474448631436,\n",
      "  accuracy_16: \n",
      "    0.9963797692990239,\n",
      "  f1_16: \n",
      "    0.8176426257924385,\n",
      "  accuracy_17: \n",
      "    0.9811535048802129,\n",
      "  f1_17: \n",
      "    0.5967272179550708,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.9979148181011535,\n",
      "  f1_19: \n",
      "    0.5461830200589762,\n",
      "}\n",
      "\n",
      "\n",
      "[8] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9945723158828749,\n",
      "  f1: \n",
      "    0.4837431366382897,\n",
      "  accuracy_0: \n",
      "    0.9978260869565218,\n",
      "  f1_0: \n",
      "    0.575875353230655,\n",
      "  accuracy_1: \n",
      "    0.9995031055900621,\n",
      "  f1_1: \n",
      "    0.6637268264371131,\n",
      "  accuracy_2: \n",
      "    0.998793256433008,\n",
      "  f1_2: \n",
      "    0.49753188822072225,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.3331172809361876,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.98301685891748,\n",
      "  f1_5: \n",
      "    0.4431483376710667,\n",
      "  accuracy_6: \n",
      "    0.9868677905944987,\n",
      "  f1_6: \n",
      "    0.7504626701417129,\n",
      "  accuracy_7: \n",
      "    0.9957941437444543,\n",
      "  f1_7: \n",
      "    0.3449765278625831,\n",
      "  accuracy_8: \n",
      "    0.9967879325643301,\n",
      "  f1_8: \n",
      "    0.5961139229006894,\n",
      "  accuracy_9: \n",
      "    0.9862999112688554,\n",
      "  f1_9: \n",
      "    0.6690975859321902,\n",
      "  accuracy_10: \n",
      "    0.9963620230700976,\n",
      "  f1_10: \n",
      "    0.6965049532816442,\n",
      "  accuracy_11: \n",
      "    0.9966370896184561,\n",
      "  f1_11: \n",
      "    0.503847257545286,\n",
      "  accuracy_12: \n",
      "    0.9973291925465838,\n",
      "  f1_12: \n",
      "    0.5686969138611858,\n",
      "  accuracy_13: \n",
      "    0.996131322094055,\n",
      "  f1_13: \n",
      "    0.4847905938605592,\n",
      "  accuracy_14: \n",
      "    0.9948181011535049,\n",
      "  f1_14: \n",
      "    0.5494621090358177,\n",
      "  accuracy_15: \n",
      "    0.99377107364685,\n",
      "  f1_15: \n",
      "    0.5451914915104416,\n",
      "  accuracy_16: \n",
      "    0.9962111801242236,\n",
      "  f1_16: \n",
      "    0.577973329152473,\n",
      "  accuracy_17: \n",
      "    0.9805057675244011,\n",
      "  f1_17: \n",
      "    0.5940433694398676,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.9977107364685004,\n",
      "  f1_19: \n",
      "    0.33295135218365884,\n",
      "}\n",
      "\n",
      "\n",
      "[8] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9584472049689441,\n",
      "  f1: \n",
      "    0.6965785626411922,\n",
      "}\n",
      "\n",
      "\n",
      "[8] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7515527950310559,\n",
      "  f1: \n",
      "    0.7512072628935678,\n",
      "  verifiability: \n",
      "    0.043478260869565216,\n",
      "}\n",
      "\n",
      "\n",
      "[8] Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] Finished epoch.\n",
      "[9] Beginning epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:42s.\n",
      "[9] Validation results:\n",
      "[9] Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9951193433895297,\n",
      "  f1: \n",
      "    0.5494736911309374,\n",
      "  accuracy_0: \n",
      "    0.9966548358473825,\n",
      "  f1_0: \n",
      "    0.49481103895686424,\n",
      "  accuracy_1: \n",
      "    0.9996362023070098,\n",
      "  f1_1: \n",
      "    0.6645087284112018,\n",
      "  accuracy_2: \n",
      "    0.999148181011535,\n",
      "  f1_2: \n",
      "    0.3331913030098593,\n",
      "  accuracy_3: \n",
      "    0.9989352262644188,\n",
      "  f1_3: \n",
      "    0.33315577651515155,\n",
      "  accuracy_4: \n",
      "    0.9997692990239574,\n",
      "  f1_4: \n",
      "    0.3332948787349029,\n",
      "  accuracy_5: \n",
      "    0.9838775510204082,\n",
      "  f1_5: \n",
      "    0.48073235464755076,\n",
      "  accuracy_6: \n",
      "    0.9871073646850045,\n",
      "  f1_6: \n",
      "    0.6353651975790507,\n",
      "  accuracy_7: \n",
      "    0.9981277728482697,\n",
      "  f1_7: \n",
      "    0.3330210030981383,\n",
      "  accuracy_8: \n",
      "    0.9967879325643301,\n",
      "  f1_8: \n",
      "    0.593063005137538,\n",
      "  accuracy_9: \n",
      "    0.9889174800354925,\n",
      "  f1_9: \n",
      "    0.6393310459064251,\n",
      "  accuracy_10: \n",
      "    0.9961579414374445,\n",
      "  f1_10: \n",
      "    0.5555334171375877,\n",
      "  accuracy_11: \n",
      "    0.997586512866016,\n",
      "  f1_11: \n",
      "    0.5128598868781911,\n",
      "  accuracy_12: \n",
      "    0.9964152617568767,\n",
      "  f1_12: \n",
      "    0.8292040930390336,\n",
      "  accuracy_13: \n",
      "    0.998340727595386,\n",
      "  f1_13: \n",
      "    0.5348912583790596,\n",
      "  accuracy_14: \n",
      "    0.995022182786158,\n",
      "  f1_14: \n",
      "    0.5815296761701371,\n",
      "  accuracy_15: \n",
      "    0.9951020408163266,\n",
      "  f1_15: \n",
      "    0.4320697422260212,\n",
      "  accuracy_16: \n",
      "    0.9965927240461402,\n",
      "  f1_16: \n",
      "    0.8315396795446247,\n",
      "  accuracy_17: \n",
      "    0.9810736468500444,\n",
      "  f1_17: \n",
      "    0.5980976053135089,\n",
      "  accuracy_18: \n",
      "    0.9993788819875776,\n",
      "  f1_18: \n",
      "    0.3332297815056436,\n",
      "  accuracy_19: \n",
      "    0.9977551020408163,\n",
      "  f1_19: \n",
      "    0.6260236302987162,\n",
      "}\n",
      "\n",
      "\n",
      "[9] Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9946126885536823,\n",
      "  f1: \n",
      "    0.5023569686224746,\n",
      "  accuracy_0: \n",
      "    0.9963886424134871,\n",
      "  f1_0: \n",
      "    0.4728261917647559,\n",
      "  accuracy_1: \n",
      "    0.9995829636202307,\n",
      "  f1_1: \n",
      "    0.6641981721928488,\n",
      "  accuracy_2: \n",
      "    0.9987666370896184,\n",
      "  f1_2: \n",
      "    0.4893850140155518,\n",
      "  accuracy_3: \n",
      "    0.9987045252883762,\n",
      "  f1_3: \n",
      "    0.33312319644839067,\n",
      "  accuracy_4: \n",
      "    0.9992901508429459,\n",
      "  f1_4: \n",
      "    0.33321498313509673,\n",
      "  accuracy_5: \n",
      "    0.9832564330079858,\n",
      "  f1_5: \n",
      "    0.468358224639288,\n",
      "  accuracy_6: \n",
      "    0.9870452528837622,\n",
      "  f1_6: \n",
      "    0.7524452509894696,\n",
      "  accuracy_7: \n",
      "    0.9959627329192546,\n",
      "  f1_7: \n",
      "    0.4231007181416193,\n",
      "  accuracy_8: \n",
      "    0.996832298136646,\n",
      "  f1_8: \n",
      "    0.597221504946837,\n",
      "  accuracy_9: \n",
      "    0.9864418811002662,\n",
      "  f1_9: \n",
      "    0.6422726995546052,\n",
      "  accuracy_10: \n",
      "    0.9960337178349601,\n",
      "  f1_10: \n",
      "    0.70539772561805,\n",
      "  accuracy_11: \n",
      "    0.996938775510204,\n",
      "  f1_11: \n",
      "    0.5443631697463068,\n",
      "  accuracy_12: \n",
      "    0.9975598935226264,\n",
      "  f1_12: \n",
      "    0.5790095181108937,\n",
      "  accuracy_13: \n",
      "    0.9966903283052352,\n",
      "  f1_13: \n",
      "    0.6058304293253945,\n",
      "  accuracy_14: \n",
      "    0.9948624667258208,\n",
      "  f1_14: \n",
      "    0.6265589659422602,\n",
      "  accuracy_15: \n",
      "    0.99377107364685,\n",
      "  f1_15: \n",
      "    0.5442576005123695,\n",
      "  accuracy_16: \n",
      "    0.9966282165039929,\n",
      "  f1_16: \n",
      "    0.5903541337516178,\n",
      "  accuracy_17: \n",
      "    0.9804702750665484,\n",
      "  f1_17: \n",
      "    0.5957710934629324,\n",
      "  accuracy_18: \n",
      "    0.9990860692102929,\n",
      "  f1_18: \n",
      "    0.49977141284615423,\n",
      "  accuracy_19: \n",
      "    0.997941437444543,\n",
      "  f1_19: \n",
      "    0.45003809174188136,\n",
      "}\n",
      "\n",
      "\n",
      "[9] Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9641881100266193,\n",
      "  f1: \n",
      "    0.7039478620532829,\n",
      "}\n",
      "\n",
      "\n",
      "[9] Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7701863354037267,\n",
      "  f1: \n",
      "    0.7699644732777262,\n",
      "  verifiability: \n",
      "    0.055900621118012424,\n",
      "}\n",
      "\n",
      "\n",
      "[9] Saving model checkpoint...\n",
      "[9] Finished epoch.\n",
      "Finished grid search! :)\n",
      "Best validation *verifiability* 0.10559006211180125 from model roberta-large_cloze_1_1e-05_6_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits.\n",
      "Best validation *accuracy* 0.782608695652174 from model roberta-large_cloze_1_1e-05_7_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits.\n",
      "Saving learning curve data...\n",
      "./saved_models/roberta-large_cloze_1_1e-05_6_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits/learning_curve_data_train.csv\n",
      "Learning curve data saved. 7990 rows saved for training, 60 rows saved for validation.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from www.model.train import train_epoch_tiered\n",
    "from www.model.eval import evaluate_tiered, save_results, save_preds, add_entity_attribute_labels\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from www.utils import print_dict, get_model_dir\n",
    "from www.model.transformers_ext import TieredModelPipeline\n",
    "from www.dataset.ann import att_to_num_classes\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "seed_val = 22 # Save random seed for reproducibility\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll keep the validation data here with a constant eval batch size\n",
    "dev_sampler = SequentialSampler(tiered_tensor_dataset['dev'])\n",
    "dev_dataloader = DataLoader(tiered_tensor_dataset['dev'], sampler=dev_sampler, batch_size=eval_batch_size)\n",
    "dev_dataset_name = subtask + '_%s_dev'\n",
    "dev_ids = [ex['example_id'] for ex in tiered_dataset['dev']]\n",
    "\n",
    "all_losses = []\n",
    "param_combos = []\n",
    "combo_names = []\n",
    "all_val_objs = []\n",
    "output_dirs = []\n",
    "best_obj = 0.0\n",
    "best_model = '<none>'\n",
    "best_dir = ''\n",
    "best_obj2 = 0.0\n",
    "best_model2 = '<none>'\n",
    "best_dir2 = ''\n",
    "\n",
    "print('Beginning grid search for the %s sub-task over %s parameter combination(s)!' % (subtask, str(len(batch_sizes) * len(learning_rates))))\n",
    "for bs in batch_sizes:\n",
    "  for lr in learning_rates:\n",
    "    print('\\nTRAINING MODEL: bs=%s, lr=%s' % (str(bs), str(lr)))\n",
    "\n",
    "    loss_values = []\n",
    "    obj_values = []\n",
    "\n",
    "    # Set up training dataset with new batch size\n",
    "    train_sampler = RandomSampler(tiered_tensor_dataset['train'])\n",
    "    train_dataloader = DataLoader(tiered_tensor_dataset['train'], sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "    # Set up model\n",
    "    config = config_class.from_pretrained(model_name,\n",
    "                                          cache_dir=os.path.join(DRIVE_PATH, 'cache'))    \n",
    "    emb = emb_class.from_pretrained(model_name,\n",
    "                                          config=config,\n",
    "                                          cache_dir=os.path.join(DRIVE_PATH, 'cache'))    \n",
    "    if torch.cuda.is_available():\n",
    "      emb.cuda()\n",
    "    device = emb.device\n",
    "    max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n",
    "    model = TieredModelPipeline(emb, max_story_length, len(att_to_num_classes), num_state_labels,\n",
    "                                config_class, model_name, device, \n",
    "                                ablation=ablation, loss_weights=loss_weights).to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n",
    "\n",
    "    train_lc_data = []\n",
    "    val_lc_data = []\n",
    "    for epoch in range(epochs):\n",
    "      # Train the model for one epoch\n",
    "      print('[%s] Beginning epoch...' % str(epoch))\n",
    "\n",
    "      epoch_loss, _ = train_epoch_tiered(model, optimizer, train_dataloader, device, seg_mode=False, \n",
    "                                         build_learning_curves=generate_learning_curve, val_dataloader=dev_dataloader, \n",
    "                                         train_lc_data=train_lc_data, val_lc_data=val_lc_data)\n",
    "      \n",
    "      # Save loss\n",
    "      loss_values.append(epoch_loss)\n",
    "\n",
    "      # Validate on dev set\n",
    "      validation_results = evaluate_tiered(model, dev_dataloader, device, [(accuracy_score, 'accuracy'), (f1_score, 'f1')], seg_mode=False, return_explanations=True)\n",
    "      metr_attr, all_pred_atts, all_atts, \\\n",
    "      metr_prec, all_pred_prec, all_prec, \\\n",
    "      metr_eff, all_pred_eff, all_eff, \\\n",
    "      metr_conflicts, all_pred_conflicts, all_conflicts, \\\n",
    "      metr_stories, all_pred_stories, all_stories, explanations = validation_results[:16]\n",
    "      explanations = add_entity_attribute_labels(explanations, tiered_dataset['dev'], list(att_to_num_classes.keys()))\n",
    "\n",
    "      print('[%s] Validation results:' % str(epoch))\n",
    "      print('[%s] Preconditions:' % str(epoch))\n",
    "      print_dict(metr_prec)\n",
    "      print('[%s] Effects:' % str(epoch))\n",
    "      print_dict(metr_eff)\n",
    "      print('[%s] Conflicts:' % str(epoch))\n",
    "      print_dict(metr_conflicts)\n",
    "      print('[%s] Stories:' % str(epoch))\n",
    "      print_dict(metr_stories)\n",
    "\n",
    "      # Save accuracy - want to maximize verifiability of tiered predictions\n",
    "      ver = metr_stories['verifiability']\n",
    "      acc = metr_stories['accuracy']\n",
    "      obj_values.append(ver)\n",
    "      \n",
    "      # Save model checkpoint\n",
    "      print('[%s] Saving model checkpoint...' % str(epoch))\n",
    "      model_param_str = get_model_dir(model_name.replace('/', '-'), subtask, bs, lr, epoch) + '_' +  '-'.join([str(lw) for lw in loss_weights]) +  '_tiered_pipeline_lc'\n",
    "      if train_spans:\n",
    "        model_param_str += 'spans'\n",
    "      if len(model.ablation) > 0:\n",
    "        model_param_str += '_ablate_'\n",
    "        model_param_str += '_'.join(model.ablation)\n",
    "      output_dir = os.path.join(DRIVE_PATH, 'saved_models', model_param_str)\n",
    "      output_dirs.append(output_dir)\n",
    "      if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "      save_results(metr_attr, output_dir, dev_dataset_name % 'attributes')\n",
    "      save_results(metr_prec, output_dir, dev_dataset_name % 'preconditions')\n",
    "      save_results(metr_eff, output_dir, dev_dataset_name % 'effects')\n",
    "      save_results(metr_conflicts, output_dir, dev_dataset_name % 'conflicts')\n",
    "      save_results(metr_stories, output_dir, dev_dataset_name % 'stories')\n",
    "      save_results(explanations, output_dir, dev_dataset_name % 'explanations')\n",
    "\n",
    "      # Just save story preds\n",
    "      save_preds(dev_ids, all_stories, all_pred_stories, output_dir, dev_dataset_name % 'stories')\n",
    "\n",
    "      emb = emb.module if hasattr(emb, 'module') else emb\n",
    "      emb.save_pretrained(output_dir)\n",
    "      torch.save(model, os.path.join(output_dir, 'classifiers.pth'))\n",
    "      tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "      if ver > best_obj:\n",
    "        best_obj = ver\n",
    "        best_model = model_param_str\n",
    "        best_dir = output_dir\n",
    "      if acc > best_obj2:\n",
    "        best_obj2 = acc\n",
    "        best_model2 = model_param_str\n",
    "        best_dir2 = output_dir        \n",
    "\n",
    "      for od in output_dirs:\n",
    "        if od != best_dir and od != best_dir2 and os.path.exists(od):\n",
    "          shutil.rmtree(od)\n",
    "\n",
    "      print('[%s] Finished epoch.' % str(epoch))\n",
    "\n",
    "    all_losses.append(loss_values)\n",
    "    all_val_objs.append(obj_values)\n",
    "    param_combos.append((bs, lr))\n",
    "    combo_names.append('bs=%s, lr=%s' % (str(bs), str(lr)))\n",
    "\n",
    "print('Finished grid search! :)')\n",
    "print('Best validation *verifiability* %s from model %s.' % (str(best_obj), best_model))\n",
    "print('Best validation *accuracy* %s from model %s.' % (str(best_obj2), best_model2))\n",
    "\n",
    "if generate_learning_curve:\n",
    "  print('Saving learning curve data...')\n",
    "  train_lc_data = [subrecord for record in train_lc_data for subrecord in record] # flatten\n",
    "  val_lc_data = [subrecord for record in val_lc_data for subrecord in record] # flatten\n",
    "\n",
    "  train_lc_data = pd.DataFrame(train_lc_data)\n",
    "  print(os.path.join(best_dir if best_dir != '<none>' else best_dir2, 'learning_curve_data_train.csv'))\n",
    "  train_lc_data.to_csv(os.path.join(best_dir if best_dir != '' else best_dir2, 'learning_curve_data_train.csv'), index=False)\n",
    "  val_lc_data = pd.DataFrame(val_lc_data)\n",
    "  val_lc_data.to_csv(os.path.join(best_dir if best_dir != '' else best_dir2, 'learning_curve_data_val.csv'), index=False)\n",
    "  print('Learning curve data saved. %s rows saved for training, %s rows saved for validation.' % (str(len(train_lc_data.index)), str(len(val_lc_data.index))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_jhT4gSBdq5"
   },
   "source": [
    "Delete all non-best model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "o7iILulEBdq6"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Delete non-best model checkpoints\n",
    "for od in output_dirs:\n",
    "  if od != best_dir and od != best_dir2 and os.path.exists(od):\n",
    "    shutil.rmtree(od)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_models/bert-large-uncased_cloze_1_1e-05_7_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits\n",
      "./saved_models/bert-large-uncased_cloze_1_1e-05_5_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits\n"
     ]
    }
   ],
   "source": [
    "print(best_dir)\n",
    "print(best_dir2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWFmGRhznl2T"
   },
   "source": [
    "### Test Models\n",
    "\n",
    "Evaluate accuracy, consistency, and verifiability on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbvpm9irn3qL"
   },
   "source": [
    "#### Load the Trained Model\n",
    "\n",
    "Load the trained model we want to probe and select the appropriate dataset. Paths to the pre-trained models presented in the paper are already provided (download links are found in GitHub repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fM_lYqw9n3qM"
   },
   "outputs": [],
   "source": [
    "from www.model.transformers_ext import TieredModelPipeline\n",
    "from www.dataset.ann import att_to_num_classes, att_to_idx, att_types\n",
    "\n",
    "#probe_model = eval_model_dir\n",
    "#probe_model = os.path.join(DRIVE_PATH, 'saved_models', probe_model)\n",
    "probe_model=best_dir\n",
    "ablation = ['attributes', 'states-logits']\n",
    "\n",
    "if 'cloze' in probe_model:\n",
    "  subtask = 'cloze'\n",
    "elif 'order' in probe_model:\n",
    "  subtask = 'order'\n",
    "  \n",
    "if subtask == 'cloze':\n",
    "  subtask_dataset = cloze_dataset_2s\n",
    "elif subtask == 'order':\n",
    "  subtask_dataset = order_dataset_2s\n",
    "\n",
    "# Load the model\n",
    "model = None\n",
    "# model = torch.load(os.path.join(probe_model, 'classifiers.pth'), map_location=torch.device('cpu'))\n",
    "model = torch.load(os.path.join(probe_model, 'classifiers.pth'))\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()\n",
    "device = model.embedding.device\n",
    "\n",
    "for layer in model.precondition_classifiers:\n",
    "  layer.eval()\n",
    "for layer in model.effect_classifiers:\n",
    "  layer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./saved_models/roberta-large_cloze_1_1e-05_6_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfXiCTA9KPjG"
   },
   "source": [
    "#### Test the Model\n",
    "\n",
    "Run inference on the testing set of TRIP. Can simply edit the top-level `for` loop if you want to run inference on other partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4255,
     "status": "ok",
     "timestamp": 1631299857967,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "FQX4bIxcKWlf",
    "outputId": "1ee7ba5e-0426-47a3-f2cc-e251bd29056f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: ./saved_models/roberta-large_cloze_1_1e-05_6_0.0-0.4-0.4-0.2-0.0_tiered_pipeline_lc_ablate_attributes_states-logits.\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[########################################################################] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:01:59s.\n",
      "\n",
      "PARTITION: test\n",
      "Stories:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7407407407407407,\n",
      "  f1: \n",
      "    0.7406059917003013,\n",
      "  verifiability: \n",
      "    0.09971509971509972,\n",
      "}\n",
      "\n",
      "\n",
      "Conflicts:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9795519980705166,\n",
      "  f1: \n",
      "    0.6476142695064642,\n",
      "}\n",
      "\n",
      "\n",
      "Preconditions:\n",
      "{\n",
      "  accuracy: \n",
      "    0.9960253395438581,\n",
      "  f1: \n",
      "    0.41978344061428163,\n",
      "  accuracy_0: \n",
      "    0.9979876090987202,\n",
      "  f1_0: \n",
      "    0.540558272677171,\n",
      "  accuracy_1: \n",
      "    0.99926136963174,\n",
      "  f1_1: \n",
      "    0.6619444103712383,\n",
      "  accuracy_2: \n",
      "    0.9991633880522769,\n",
      "  f1_2: \n",
      "    0.33319383965767496,\n",
      "  accuracy_3: \n",
      "    0.9993442771220549,\n",
      "  f1_3: \n",
      "    0.3332240103442166,\n",
      "  accuracy_4: \n",
      "    0.9998794072868147,\n",
      "  f1_4: \n",
      "    0.33331323333584584,\n",
      "  accuracy_5: \n",
      "    0.9889280815206741,\n",
      "  f1_5: \n",
      "    0.349403873331483,\n",
      "  accuracy_6: \n",
      "    0.9870136721988574,\n",
      "  f1_6: \n",
      "    0.6203380977446721,\n",
      "  accuracy_7: \n",
      "    0.9984624429068873,\n",
      "  f1_7: \n",
      "    0.33307687665942554,\n",
      "  accuracy_8: \n",
      "    0.9973620343990715,\n",
      "  f1_8: \n",
      "    0.5695990539178056,\n",
      "  accuracy_9: \n",
      "    0.9871342649120427,\n",
      "  f1_9: \n",
      "    0.6199149581435904,\n",
      "  accuracy_10: \n",
      "    0.9977765718506459,\n",
      "  f1_10: \n",
      "    0.5147801278373505,\n",
      "  accuracy_11: \n",
      "    0.9985453503972023,\n",
      "  f1_11: \n",
      "    0.4389107661797899,\n",
      "  accuracy_12: \n",
      "    0.9969098117246266,\n",
      "  f1_12: \n",
      "    0.7756644548734039,\n",
      "  accuracy_13: \n",
      "    0.9984925910851836,\n",
      "  f1_13: \n",
      "    0.3330819090145675,\n",
      "  accuracy_14: \n",
      "    0.9972037564630157,\n",
      "  f1_14: \n",
      "    0.5134218750458936,\n",
      "  accuracy_15: \n",
      "    0.9957792550385143,\n",
      "  f1_15: \n",
      "    0.3693560921921666,\n",
      "  accuracy_16: \n",
      "    0.9970756267052563,\n",
      "  f1_16: \n",
      "    0.728192672419516,\n",
      "  accuracy_17: \n",
      "    0.9861544491174121,\n",
      "  f1_17: \n",
      "    0.5886919885929941,\n",
      "  accuracy_18: \n",
      "    0.999449795746092,\n",
      "  f1_18: \n",
      "    0.49986241108552004,\n",
      "  accuracy_19: \n",
      "    0.9985830356200727,\n",
      "  f1_19: \n",
      "    0.5479291678412737,\n",
      "}\n",
      "\n",
      "\n",
      "Effects:\n",
      "{\n",
      "  accuracy: \n",
      "    0.995695593843742,\n",
      "  f1: \n",
      "    0.4132511304463613,\n",
      "  accuracy_0: \n",
      "    0.9978594793409609,\n",
      "  f1_0: \n",
      "    0.5290591216461497,\n",
      "  accuracy_1: \n",
      "    0.9991859991859992,\n",
      "  f1_1: \n",
      "    0.6615696482865329,\n",
      "  accuracy_2: \n",
      "    0.9988317580910173,\n",
      "  f1_2: \n",
      "    0.33313851254960075,\n",
      "  accuracy_3: \n",
      "    0.9992462955425918,\n",
      "  f1_3: \n",
      "    0.33320766856671796,\n",
      "  accuracy_4: \n",
      "    0.9993593512112031,\n",
      "  f1_4: \n",
      "    0.33322652432166855,\n",
      "  accuracy_5: \n",
      "    0.9887095072280258,\n",
      "  f1_5: \n",
      "    0.36978463118777793,\n",
      "  accuracy_6: \n",
      "    0.9867121904158941,\n",
      "  f1_6: \n",
      "    0.7363063506135155,\n",
      "  accuracy_7: \n",
      "    0.9975504605134234,\n",
      "  f1_7: \n",
      "    0.3329245761187685,\n",
      "  accuracy_8: \n",
      "    0.9972188305521639,\n",
      "  f1_8: \n",
      "    0.5741681775382701,\n",
      "  accuracy_9: \n",
      "    0.9862448936523011,\n",
      "  f1_9: \n",
      "    0.6162998575501254,\n",
      "  accuracy_10: \n",
      "    0.9977388866277755,\n",
      "  f1_10: \n",
      "    0.629732250962439,\n",
      "  accuracy_11: \n",
      "    0.9975730716471457,\n",
      "  f1_11: \n",
      "    0.4194667328545358,\n",
      "  accuracy_12: \n",
      "    0.9980328313661647,\n",
      "  f1_12: \n",
      "    0.5473792534646311,\n",
      "  accuracy_13: \n",
      "    0.9972037564630157,\n",
      "  f1_13: \n",
      "    0.3441183842472075,\n",
      "  accuracy_14: \n",
      "    0.9971811453292935,\n",
      "  f1_14: \n",
      "    0.5161958817650153,\n",
      "  accuracy_15: \n",
      "    0.9943170683911424,\n",
      "  f1_15: \n",
      "    0.5290284656164567,\n",
      "  accuracy_16: \n",
      "    0.9968118301451635,\n",
      "  f1_16: \n",
      "    0.5397256630728383,\n",
      "  accuracy_17: \n",
      "    0.9861393750282639,\n",
      "  f1_17: \n",
      "    0.5884298853725505,\n",
      "  accuracy_18: \n",
      "    0.9992387584980178,\n",
      "  f1_18: \n",
      "    0.33320641144056345,\n",
      "  accuracy_19: \n",
      "    0.9987563876452765,\n",
      "  f1_19: \n",
      "    0.42124850502894784,\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from www.model.eval import evaluate_tiered, save_results, save_preds, list_comparison, add_entity_attribute_labels\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "metrics = [(accuracy_score, 'accuracy'), (precision_score, 'precision'), (recall_score, 'recall'), (f1_score, 'f1')]\n",
    "import numpy as np\n",
    "from www.utils import print_dict\n",
    "\n",
    "print('Testing model: %s.' % probe_model)\n",
    "train_spans = False\n",
    "if train_spans:\n",
    "  tiered_dataset = get_story_spans_2s(tiered_dataset, train_only=True)\n",
    "  tiered_dataset['train'] = [ex for ex in tiered_dataset['train'] if ex['label'] != -1] # For now, ignore examples where both stories are plausible :(\n",
    "\n",
    "# May alter this depending on which partition(s) you want to run inference on\n",
    "for p in tiered_dataset:\n",
    "  if p != 'test':\n",
    "    continue\n",
    "\n",
    "  p_dataset = tiered_dataset[p]\n",
    "  p_tensor_dataset = tiered_tensor_dataset[p]\n",
    "  p_sampler = SequentialSampler(p_tensor_dataset)\n",
    "  p_dataloader = DataLoader(p_tensor_dataset, sampler=p_sampler, batch_size=16)\n",
    "  dev_dataset_name = subtask + '_%s_' + p\n",
    "  p_ids = [ex['example_id'] for ex in tiered_dataset[p]]\n",
    "\n",
    "  # Get preds and metrics on this partition\n",
    "  metr_attr, all_pred_atts, all_atts, \\\n",
    "  metr_prec, all_pred_prec, all_prec, \\\n",
    "  metr_eff, all_pred_eff, all_eff, \\\n",
    "  metr_conflicts, all_pred_conflicts, all_conflicts, \\\n",
    "  metr_stories, all_pred_stories, all_stories, explanations = evaluate_tiered(model, p_dataloader, device, [(accuracy_score, 'accuracy'), (f1_score, 'f1')], seg_mode=False, return_explanations=True)\n",
    "  explanations = add_entity_attribute_labels(explanations, tiered_dataset[p], list(att_to_num_classes.keys()))\n",
    "\n",
    "  save_results(metr_attr, probe_model, dev_dataset_name % 'attributes')\n",
    "  save_results(metr_prec, probe_model, dev_dataset_name % 'preconditions')\n",
    "  save_results(metr_eff, probe_model, dev_dataset_name % 'effects')\n",
    "  save_results(metr_conflicts, probe_model, dev_dataset_name % 'conflicts')\n",
    "  save_results(metr_stories, probe_model, dev_dataset_name % 'stories')\n",
    "  save_results(explanations, probe_model, dev_dataset_name % 'explanations')\n",
    "\n",
    "  print('\\nPARTITION: %s' % p)\n",
    "  print('Stories:')\n",
    "  print_dict(metr_stories)\n",
    "  print('Conflicts:')\n",
    "  print_dict(metr_conflicts)\n",
    "  print('Preconditions:')\n",
    "  print_dict(metr_prec)\n",
    "  print('Effects:')\n",
    "  print_dict(metr_eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON1UAnbc8OOE"
   },
   "source": [
    "#### Add Consistency Metric to Model Results\n",
    "The intermediate conistency metric isn't included in the originally calculated metrics. This block adds the consistency metric to pre-existing model directory based on the tiered predictions. Generates a new `results_cloze_stories_final_[partition].json` file that includes the consistency metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1631299967551,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "k1obFel58pd-",
    "outputId": "f8cc0c5a-756b-4879-deb2-3aa8489474c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 consistent preds in dev (versus 34 verifiable)\n",
      "Found 73 consistent preds in test (versus 35 verifiable)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "best_dir=probe_model\n",
    "model_directories = [best_dir]\n",
    "\n",
    "partitions = ['dev', 'test']\n",
    "expl_fname = 'results_cloze_explanations_%s.json'\n",
    "endtask_fname = 'results_cloze_stories_%s.json'\n",
    "endtask_fname_new = 'results_cloze_stories_final_%s.json'\n",
    "for md in model_directories:\n",
    "  for p in partitions:\n",
    "    explanations = json.load(open(os.path.join(DRIVE_PATH, md, expl_fname % p), 'r'))\n",
    "    #explanations = json.load(open(best_dir, 'r'))\n",
    "    endtask_results = json.load(open(os.path.join(DRIVE_PATH, md, endtask_fname % p), 'r'))\n",
    "    #endtask_results = json.load(open(best_dir, 'r'))\n",
    "    consistent_preds = 0\n",
    "    verifiable_preds = 0\n",
    "    total = 0\n",
    "    for expl in explanations:\n",
    "      if expl['valid_explanation']:\n",
    "        verifiable_preds += 1\n",
    "      if expl['story_pred'] == expl['story_label']:\n",
    "        if len(expl['conflict_pred']) == len(expl['conflict_label']) and expl['conflict_pred'][0] == expl['conflict_label'][0] and expl['conflict_pred'][1] == expl['conflict_label'][1]:\n",
    "          expl['consistent'] = True\n",
    "          consistent_preds += 1\n",
    "        else:\n",
    "          expl['consistent'] = False\n",
    "      total += 1\n",
    "\n",
    "    endtask_results['consistency'] = float(consistent_preds) / total\n",
    "    print('Found %s consistent preds in %s (versus %s verifiable)' % (str(consistent_preds), p, str(verifiable_preds)))\n",
    "    json.dump(explanations, open(os.path.join(DRIVE_PATH, md, (expl_fname % p).replace('explanations', 'explanations_consistency')), 'w'))\n",
    "    json.dump(endtask_results, open(os.path.join(DRIVE_PATH, md, endtask_fname_new % p), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7C0wy5Dvtv2"
   },
   "source": [
    "\n",
    "# Conversational Entailment (CE) Results\n",
    "\n",
    "Code for the coherence experiments on CE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKxPGbnWbdIy"
   },
   "outputs": [],
   "source": [
    "if task_name != 'ce':\n",
    "  raise ValueError('Please configure task_name in first cell to \"ce\" to run CE results!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7MADzgfvtv3"
   },
   "source": [
    "## Load Conversational Entailment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1631301900364,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "nSnNY2zbvtv3",
    "outputId": "fb02836c-d14b-435a-96d7-e40dac522e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 40 dialog sources for training and validation.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "cache_train = os.path.join(DRIVE_PATH, 'all_data/ConvEnt/ConvEnt_train_resplit.json')\n",
    "cache_dev = os.path.join(DRIVE_PATH,'all_data/ConvEnt/ConvEnt_dev_resplit.json')\n",
    "cache_test = os.path.join(DRIVE_PATH,'all_data/ConvEnt/ConvEnt_test_resplit.json')\n",
    "ConvEnt_train = json.load(open(cache_train))\n",
    "ConvEnt_dev = json.load(open(cache_dev))\n",
    "ConvEnt_test = json.load(open(cache_test))\n",
    "\n",
    "# Combine train and dev and do cross-validation\n",
    "cache_folds = os.path.join(DRIVE_PATH,'all_data/ConvEnt/ConvEnt_folds.pkl') # Folds used for results presented in paper\n",
    "ConvEnt_train = ConvEnt_train + ConvEnt_dev\n",
    "train_sources = list(set([ex['dialog_source'] for ex in ConvEnt_train]))\n",
    "print(\"Reserved %s dialog sources for training and validation.\" % len(train_sources))\n",
    "\n",
    "no_folds = 8\n",
    "if not os.path.exists(cache_folds):\n",
    "  folds = []\n",
    "  for k in range(no_folds):\n",
    "    folds.append(np.random.choice(train_sources, size=5, replace=False))\n",
    "    train_sources = [s for s in train_sources if s not in folds[-1]]\n",
    "  assert len(train_sources) == 0\n",
    "  print(folds)\n",
    "  pickle.dump(folds, open(cache_folds, 'wb'))\n",
    "else:\n",
    "  folds = pickle.load(open(cache_folds, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1631301900480,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "TBYhEsdovtv4",
    "outputId": "c1c90590-7443-4f03-96e6-07e4ac429161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples: 703\n",
      "dev examples: 110\n",
      "test examples: 172\n"
     ]
    }
   ],
   "source": [
    "print('train examples:', len(ConvEnt_train))\n",
    "print('dev examples:', len(ConvEnt_dev))\n",
    "print('test examples:', len(ConvEnt_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msxt3xAhvtv6"
   },
   "source": [
    "## Featurize Conversational Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBfqFw82vtv7"
   },
   "outputs": [],
   "source": [
    "from www.dataset.featurize import add_bert_features_ConvEnt, get_tensor_dataset\n",
    "import pickle\n",
    "seq_length = 128\n",
    "\n",
    "ConvEnt_train = add_bert_features_ConvEnt(ConvEnt_train, tokenizer, seq_length, add_segment_ids=True)\n",
    "ConvEnt_dev = add_bert_features_ConvEnt(ConvEnt_dev, tokenizer, seq_length, add_segment_ids=True)\n",
    "ConvEnt_test = add_bert_features_ConvEnt(ConvEnt_test, tokenizer, seq_length, add_segment_ids=True)\n",
    "\n",
    "ConvEnt_train_folds = [[] for _ in range(no_folds)]\n",
    "ConvEnt_dev_folds = [[] for _ in range(no_folds)]\n",
    "for k in range(no_folds):\n",
    "  ConvEnt_train_folds[k] = [ex for ex in ConvEnt_train if ex['dialog_source'] not in folds[k]]\n",
    "  ConvEnt_dev_folds[k] = [ex for ex in ConvEnt_train if ex['dialog_source'] in folds[k]]\n",
    "\n",
    "  if debug:\n",
    "    ConvEnt_train_folds[k] = ConvEnt_train_folds[k][:10]\n",
    "    ConvEnt_dev_folds[k] = ConvEnt_dev_folds[k][:10]\n",
    "\n",
    "if debug:\n",
    "  ConvEnt_train = ConvEnt_train[:10]\n",
    "  ConvEnt_dev = ConvEnt_dev[:10]\n",
    "  ConvEnt_test = ConvEnt_test[:10]\n",
    "\n",
    "ConvEnt_train_tensor = get_tensor_dataset(ConvEnt_train, label_key='label', add_segment_ids=True)\n",
    "ConvEnt_test_tensor = get_tensor_dataset(ConvEnt_test, label_key='label', add_segment_ids=True)\n",
    "\n",
    "# Training sets for each validation fold\n",
    "ConvEnt_train_folds_tensor = [get_tensor_dataset(ConvEnt_train_folds[k], label_key='label', add_segment_ids=True) for k in range(no_folds)]\n",
    "ConvEnt_dev_folds_tensor = [get_tensor_dataset(ConvEnt_dev_folds[k], label_key='label', add_segment_ids=True) for k in range(no_folds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1631301488436,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "2Wtek-Tmvtv7",
    "outputId": "9d3f5110-edce-4d35-d4b2-23891d4da280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples: 10\n",
      "dev examples: 10\n",
      "test examples: 10\n"
     ]
    }
   ],
   "source": [
    "print('train examples:', len(ConvEnt_train))\n",
    "print('dev examples:', len(ConvEnt_dev))\n",
    "print('test examples:', len(ConvEnt_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn5Ywwvxvtv6"
   },
   "source": [
    "## Train Models on Conversational Entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytbj9Uxxvtv7"
   },
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9YL5qnRvtv7"
   },
   "source": [
    "#### Configure Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fC5iz07Fvtv8"
   },
   "outputs": [],
   "source": [
    "batch_sizes = [config_batch_size]\n",
    "learning_rates = [config_lr]\n",
    "epochs = config_epochs\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jss8T2xzvtv8"
   },
   "source": [
    "#### Grid Search and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141231,
     "status": "ok",
     "timestamp": 1631301166877,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "5QOdgrufvtv8",
    "outputId": "5d20813e-882f-4896-fe6e-e03db305d6d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning grid search for ConvEnt over 1 parameter combination(s)!\n",
      "\n",
      "TRAINING MODEL: bs=1, lr=1e-05\n",
      "Beginning fold 1/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.5,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 2/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 3/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.3,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 4/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.6,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 5/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.4,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 6/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.3,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 7/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.5,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Beginning fold 8/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.4,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Finished epoch.\n",
      "Top performing param combos:\n",
      "[((1, 1e-05, 0), 0.46249999999999997)]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from www.model.train import train_epoch\n",
    "from www.model.eval import evaluate, save_results, save_preds\n",
    "from sklearn.metrics import accuracy_score\n",
    "from www.utils import print_dict, get_model_dir\n",
    "from collections import Counter\n",
    "\n",
    "seed_val = 22 # Save random seed for reproducibility\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "assert len(batch_sizes) == 1\n",
    "train_fold_sampler = [RandomSampler(f) for f in ConvEnt_train_folds_tensor]\n",
    "train_fold_dataloader = [DataLoader(f, sampler=train_fold_sampler[i], batch_size=batch_sizes[0]) for i, f in enumerate(ConvEnt_train_folds_tensor)]\n",
    "\n",
    "dev_fold_sampler = [SequentialSampler(f) for f in ConvEnt_dev_folds_tensor]\n",
    "dev_fold_dataloader = [DataLoader(f, sampler=dev_fold_sampler[i], batch_size=eval_batch_size) for i, f in enumerate(ConvEnt_dev_folds_tensor)]\n",
    "\n",
    "all_val_accs = Counter()\n",
    "print('Beginning grid search for ConvEnt over %s parameter combination(s)!' % (str(len(batch_sizes) * len(learning_rates))))\n",
    "for bs in batch_sizes:\n",
    "  for lr in learning_rates:\n",
    "    print('\\nTRAINING MODEL: bs=%s, lr=%s' % (str(bs), str(lr)))\n",
    "\n",
    "    for k in range(no_folds):\n",
    "      print('Beginning fold %s/%s...' % (str(k+1), str(no_folds)))\n",
    "\n",
    "      # Set up model\n",
    "      if 'mnli' not in mode:\n",
    "        model = model_class.from_pretrained(model_name, \n",
    "                                            cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "      else:\n",
    "        config = config_class.from_pretrained(model_name.replace('-mnli',''),\n",
    "                                        num_labels=3,\n",
    "                                        cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "        model = model_class.from_pretrained(model_name, \n",
    "                                            config=config,\n",
    "                                            cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "        config.num_labels = 2\n",
    "        model.num_labels = 2\n",
    "        model.classifier = cls_head_class(config=config) # Need to bring in a classification head for only 2 labels\n",
    "    \n",
    "      model.cuda()\n",
    "      device = model.device \n",
    "\n",
    "      # Set up optimizer\n",
    "      optimizer = AdamW(model.parameters(), lr=lr)\n",
    "      total_steps = len(train_fold_dataloader[k]) * epochs\n",
    "      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n",
    "\n",
    "      for epoch in range(epochs):\n",
    "        # Train the model for one epoch\n",
    "        print('[%s] Beginning epoch...' % str(epoch))\n",
    "\n",
    "        epoch_loss, _ = train_epoch(model, optimizer, train_fold_dataloader[k], device, seg_mode=True if 'roberta' not in mode else False)\n",
    "        \n",
    "        # Validate on dev set\n",
    "        results, _, _ = evaluate(model, dev_fold_dataloader[k], device, [(accuracy_score, 'accuracy')], seg_mode=True if 'roberta' not in mode else False)\n",
    "        print('[%s] Validation results:' % str(epoch))\n",
    "        print_dict(results)\n",
    "\n",
    "        # Save accuracy\n",
    "        acc = results['accuracy']\n",
    "        if (bs, lr, epoch) in all_val_accs:\n",
    "          all_val_accs[(bs, lr, epoch)] += acc\n",
    "        else:\n",
    "          all_val_accs[(bs, lr, epoch)] = acc\n",
    "        \n",
    "      model.cpu()\n",
    "      del model\n",
    "      del optimizer\n",
    "      del results\n",
    "      del scheduler\n",
    "      del total_steps\n",
    "\n",
    "      print('[%s] Finished epoch.' % str(epoch))\n",
    "\n",
    "for k in all_val_accs:\n",
    "  all_val_accs[k] /= no_folds\n",
    "\n",
    "print('Top performing param combos:')\n",
    "print(all_val_accs.most_common(5))\n",
    "\n",
    "save_fname = os.path.join(DRIVE_PATH, 'saved_models/%s_ConvEnt_xval_%s.pkl' % (model_name.replace('/','-'), '_'.join([str(lr) for lr in learning_rates])))\n",
    "pickle.dump(all_val_accs, open(save_fname, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XhSrDlpI0aH"
   },
   "source": [
    "#### Re-Train Best Model from Cross-Validation\n",
    "\n",
    "Re-train a model with the best parameters from the search above. If this isn't run directly after the above cell, replace `save_fname.split('/'[-1])` in `xval_fnames` with the name of the `pkl` file previously generated in the `saved_models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23469,
     "status": "ok",
     "timestamp": 1631301377078,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "RI_AWgyFIzUm",
    "outputId": "f29073fc-87b1-4405-d4bd-e99e17624a00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "[0] Saving model checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('drive/My Drive/Colab Notebooks/Research/TRIP_replication/saved_models/roberta-large_ConvEnt_1_1e-05_0_xval/vocab.json',\n",
       " 'drive/My Drive/Colab Notebooks/Research/TRIP_replication/saved_models/roberta-large_ConvEnt_1_1e-05_0_xval/merges.txt')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from www.model.train import train_epoch\n",
    "from www.model.eval import evaluate, save_results, save_preds\n",
    "from sklearn.metrics import accuracy_score\n",
    "from www.utils import print_dict, get_model_dir\n",
    "from collections import Counter\n",
    "\n",
    "# Re-train the model with the best parameters from the grid search/cross-validation (with all folds)\n",
    "xval_fnames = []\n",
    "xval_fnames.append(save_fname.split('/')[-1])\n",
    "\n",
    "xval_results = Counter()\n",
    "for fname in xval_fnames:\n",
    "  xval_results += pickle.load(open(os.path.join(DRIVE_PATH, 'saved_models/', fname), 'rb'))\n",
    "\n",
    "batch_size, learning_rate, epochs = xval_results.most_common(1)[0][0]\n",
    "epochs += 1\n",
    "\n",
    "# Set up model\n",
    "if 'mnli' not in mode:\n",
    "  model = model_class.from_pretrained(model_name, \n",
    "                                      cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "else:\n",
    "  config = config_class.from_pretrained(model_name.replace('-mnli',''),\n",
    "                                  num_labels=3,\n",
    "                                  cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "  model = model_class.from_pretrained(model_name, \n",
    "                                      config=config,\n",
    "                                      cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "  config.num_labels = 2\n",
    "  model.num_labels = 2\n",
    "  model.classifier = cls_head_class(config=config) # Need to bring in a classification head for only 2 labels\n",
    "\n",
    "model.cuda()\n",
    "device = model.device \n",
    "\n",
    "train_sampler = RandomSampler(ConvEnt_train_tensor)\n",
    "train_dataloader = DataLoader(ConvEnt_train_tensor, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print('[%s] Beginning epoch...' % str(epoch))\n",
    "  epoch_loss, _ = train_epoch(model, optimizer, train_dataloader, device, seg_mode=True if 'roberta' not in mode else False)\n",
    "\n",
    "print('[%s] Saving model checkpoint...' % str(epoch))\n",
    "model_param_str = get_model_dir(model_name.replace('/','-'), 'ConvEnt', batch_size, learning_rate, epoch) + '_xval'\n",
    "output_dir = os.path.join(DRIVE_PATH, 'saved_models', model_param_str)\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "model = model.module if hasattr(model, 'module') else model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CADDFieTvtv9"
   },
   "source": [
    "## Test Models on Conversational Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14850,
     "status": "ok",
     "timestamp": 1631301512340,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "-NJAbMNRvtv9",
    "outputId": "a493868a-c762-4279-b73b-7b6e7b20e148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: roberta-large_ConvEnt_1_1e-05_0_xval.\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "Results (test):\n",
      "{\n",
      "  accuracy: \n",
      "    0.7,\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from www.model.eval import evaluate, save_results, save_preds\n",
    "from sklearn.metrics import accuracy_score\n",
    "from www.utils import print_dict, get_model_dir\n",
    "\n",
    "best_model = eval_model_dir\n",
    "\n",
    "\n",
    "best_model = os.path.join(DRIVE_PATH, 'saved_models', best_model)\n",
    "\n",
    "# Load the model\n",
    "model = model_class.from_pretrained(best_model)\n",
    "model.cuda()\n",
    "device = model.device\n",
    "\n",
    "# Select appropriate dataset\n",
    "if 'cloze' in best_model:\n",
    "  subtask = 'cloze'\n",
    "elif 'order' in best_model:\n",
    "  subtask = 'order'\n",
    "\n",
    "test_sampler = SequentialSampler(ConvEnt_test_tensor)\n",
    "test_dataloader = DataLoader(ConvEnt_test_tensor, sampler=test_sampler, batch_size=128)\n",
    "test_dataset_name = '%s_%s' % ('ConvEnt', 'test')\n",
    "test_ids = [str(ex['example_id']) for ex in ConvEnt_test]\n",
    "\n",
    "print('Testing model: %s.' % best_model.split('/')[-1])\n",
    "\n",
    "results, preds, labels = evaluate(model, test_dataloader, device, [(accuracy_score, 'accuracy')], seg_mode=True if 'roberta' not in mode else False)\n",
    "save_results(results, best_model, test_dataset_name)\n",
    "save_preds(test_ids, labels, preds, best_model, test_dataset_name)\n",
    "\n",
    "print('Results (%s):' % p)\n",
    "print_dict(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy_A5hR9vtv9"
   },
   "source": [
    "## Coherence Checks on Conversational Entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GO1JOxnIvtv9"
   },
   "source": [
    "### Load and Featurize Span Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOsSHJnCvtv-"
   },
   "outputs": [],
   "source": [
    "from www.dataset.featurize import add_bert_features_ConvEnt, get_tensor_dataset\n",
    "from www.dataset.prepro import get_ConvEnt_spans\n",
    "import pickle\n",
    "seq_length = 128\n",
    "\n",
    "merged_file = os.path.join(DRIVE_PATH, 'all_data/ConvEnt/ConvEnt_test_annotation_merged2.json')\n",
    "ConvEnt_test = json.load(open(merged_file))\n",
    "\n",
    "ConvEnt_test = add_bert_features_ConvEnt(ConvEnt_test, tokenizer, seq_length, add_segment_ids=True)\n",
    "\n",
    "if debug:\n",
    "  ConvEnt_test = ConvEnt_test[:10]\n",
    "\n",
    "# Some of the annotated examples are no longer in the test set :(\n",
    "# ConvEnt_test = [ex for ex in ConvEnt_test if ex['id'] in test_ids]\n",
    "\n",
    "# Make span versions of the datasets\n",
    "ConvEnt_test_spans = get_ConvEnt_spans(ConvEnt_test)\n",
    "\n",
    "# Add BERT features\n",
    "ConvEnt_test_tensor = get_tensor_dataset(ConvEnt_test, label_key='label', add_segment_ids=True)\n",
    "ConvEnt_test_spans_tensor = get_tensor_dataset(ConvEnt_test_spans, label_key='label', add_segment_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAtipHPtvtv-"
   },
   "source": [
    "### Load the Trained Model\n",
    "\n",
    "Load the trained model we want to probe and select the appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCssPtg_vtv-"
   },
   "outputs": [],
   "source": [
    "probe_model = eval_model_dir\n",
    "probe_model = os.path.join(DRIVE_PATH, 'saved_models', probe_model)\n",
    "\n",
    "# Load the model\n",
    "model = model_class.from_pretrained(probe_model)\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()\n",
    "device = model.device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIPh3HSXvtv-"
   },
   "source": [
    "#### Load Trained Model's Base Predictions\n",
    "\n",
    "For comparison, we also want the preds and labels for the previous level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1631301931718,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "U9lHmA2evtv-",
    "outputId": "60d88a06-7376-4bd6-edcc-d846cf8aa7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['73', '74', '75', '76', '77', '78', '79', '80', '81', '82'])\n"
     ]
    }
   ],
   "source": [
    "from www.model.eval import load_preds\n",
    "from www.utils import print_dict\n",
    "\n",
    "preds_base = {}\n",
    "preds_base['test'] = load_preds(os.path.join(probe_model, 'preds_ConvEnt_test.tsv'))\n",
    "print(preds_base['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maZGCMvMvtv-"
   },
   "source": [
    "### Check a Model\n",
    "\n",
    "Will print out strict and lenient coherence metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3056,
     "status": "ok",
     "timestamp": 1631302156247,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "tl_M9hnqvtv-",
    "outputId": "1143ab4e-d54d-471f-f186-ae09646ea059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: drive/My Drive/Colab Notebooks/Research/TRIP_replication/saved_models/roberta-large_ConvEnt_1_1e-05_0_xval.\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:02s.\n",
      "\n",
      "PARTITION: test\n",
      "{\n",
      "  lenient_coherence: \n",
      "    0.4269000933706816,\n",
      "  strict_coherence: \n",
      "    0.4,\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from www.model.eval import evaluate, save_results, save_preds, list_comparison\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "metrics = [(accuracy_score, 'accuracy'), (precision_score, 'precision'), (recall_score, 'recall'), (f1_score, 'f1')]\n",
    "import numpy as np\n",
    "from www.utils import print_dict\n",
    "\n",
    "def is_polarized(smax, thres):\n",
    "  return (abs(smax[0] - smax[1]) >= thres)\n",
    "\n",
    "print('Testing model: %s.' % probe_model)\n",
    "\n",
    "all_results = {}\n",
    "p = 'test'\n",
    "\n",
    "p_dataset = ConvEnt_test_spans\n",
    "p_tensor_dataset = ConvEnt_test_spans_tensor\n",
    "p_sampler = SequentialSampler(p_tensor_dataset)\n",
    "p_dataloader = DataLoader(p_tensor_dataset, sampler=p_sampler, batch_size=512)\n",
    "p_dataset_name = '%s_spans_%s' % ('ConvEnt', p)\n",
    "p_dataset_name_co = '%s_consistent_%s' % ('ConvEnt', p)\n",
    "p_dataset_name_bp = '%s_breakpoints_%s' % ('ConvEnt', p)\n",
    "p_dataset_name_ev = '%s_evidence_%s' % ('ConvEnt', p)\n",
    "p_dataset_name_coh = '%s_coherent_%s' % ('ConvEnt', p)\n",
    "p_ids = [str(ex['example_id']) for ex in ConvEnt_test_spans]\n",
    "p_labels = [ex['label'] for ex in ConvEnt_test_spans]\n",
    "\n",
    "# Get span preds and save metrics\n",
    "results, preds, labels = evaluate(model, p_dataloader, device, metrics, seg_mode=True if 'roberta' not in mode else False)\n",
    "save_results(results, probe_model, p_dataset_name)\n",
    "save_preds(p_ids, labels, preds, probe_model, p_dataset_name)\n",
    "\n",
    "# Convert substory preds into breakpoint preds for each example\n",
    "ids_base = [str(ex['example_id']) for ex in ConvEnt_test]\n",
    "\n",
    "id_to_pred = {k: v for k,v in zip(p_ids, preds)}\n",
    "id_to_label = {k: v for k,v in zip(p_ids, p_labels)}\n",
    "\n",
    "preds_entailment = []\n",
    "labels_entailment = []\n",
    "preds_consistent = []\n",
    "preds_breakpoint = []\n",
    "labels_breakpoint = []\n",
    "preds_evidence = []\n",
    "labels_evidence = []    \n",
    "span_accuracies = []\n",
    "span_accuracies_strict = []\n",
    "preds_coherent = []\n",
    "\n",
    "for i, exid in enumerate(ids_base):\n",
    "  ex = ConvEnt_test[i]\n",
    "  ex['length'] = len(ex['turns'])\n",
    "\n",
    "  label_entailment = preds_base[p][exid]['label']\n",
    "  pred_entailment = preds_base[p][exid]['pred']\n",
    "  labels_entailment.append(label_entailment)\n",
    "  preds_entailment.append(pred_entailment)\n",
    "\n",
    "  # Get ground truth breakpoint and evidence\n",
    "  label_breakpoint = ex['conflict_pair'][1] if ex['conflict_pair'] is not None and len(ex['conflict_pair']) > 0 else 0\n",
    "  labels_breakpoint.append(label_breakpoint)\n",
    "  if label_breakpoint > 0:\n",
    "    label_ev = ex['conflict_pair'][0]\n",
    "  else:\n",
    "    label_ev = -1\n",
    "  labels_evidence.append(label_ev)\n",
    "\n",
    "  # Check consistency - any span that entails the hypothesis' superspans should also entail\n",
    "  pred_consistent = True\n",
    "  span_accuracy = 0.0\n",
    "  span_accuracy_strict = 0.0\n",
    "  pred_coherent = True\n",
    "  \n",
    "  no_spans = 0\n",
    "  for sp1 in range(ex['length']):\n",
    "    if not pred_consistent:\n",
    "      break\n",
    "\n",
    "    for sp2 in range(sp1, ex['length']):\n",
    "      if not pred_consistent:\n",
    "        break\n",
    "\n",
    "      span_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n",
    "      span_label = id_to_label[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n",
    "\n",
    "      if span_pred == span_label:\n",
    "        span_accuracy += 1.0\n",
    "        if label_entailment == pred_entailment:\n",
    "            span_accuracy_strict += 1.0\n",
    "      else:\n",
    "        pred_coherent = False\n",
    "      no_spans += 1\n",
    "      # print('%s:%s\\t%s\\t(%s, %s)' % (str(sp1), str(sp2), str(span_pred), str(span_prob[0]), str(span_prob[1])))      \n",
    "\n",
    "      if span_pred == 1:\n",
    "        if pred_entailment == 1:\n",
    "          for sp3 in range(sp1+1):\n",
    "            if not pred_consistent:\n",
    "              break\n",
    "\n",
    "            for sp4 in range(sp2, ex['length']):\n",
    "              if not pred_consistent:\n",
    "                break\n",
    "\n",
    "              sspan_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp3), str(sp4))]\n",
    "\n",
    "              if sspan_pred == 0:\n",
    "                pred_consistent = False\n",
    "                break\n",
    "        elif pred_entailment == 0:\n",
    "          pred_consistent = False\n",
    "\n",
    "  preds_consistent.append(1 if pred_consistent else 0)\n",
    "  span_accuracies.append(span_accuracy / no_spans)\n",
    "  span_accuracies_strict.append(span_accuracy_strict / no_spans)\n",
    "  preds_coherent.append(1 if pred_coherent else 0)\n",
    "\n",
    "  # Check pred. breakpoint (verifiability) - will be first sentence where the model prediction becomes polarized, i.e., confidence > threshold\n",
    "  pred_breakpoint = 0 # For now, 0 means -1, i.e., stories are entirely plausible - this shouldn't happen but it will (inconsistent?)\n",
    "  for ss in range(1, ex['length']):\n",
    "    if id_to_pred[exid + '-sp%s:%s' % (str(0), str(ss))] == 1:\n",
    "      pred_breakpoint = ss\n",
    "      break\n",
    "  preds_breakpoint.append(pred_breakpoint)\n",
    "\n",
    "  # Check pred. evidence (verifiability)\n",
    "  if pred_breakpoint > 0:\n",
    "    pred_evidence = -1 \n",
    "    for ss in range(0, pred_breakpoint+1):\n",
    "      if id_to_pred[exid + '-sp%s:%s' % (str(0), str(ss))] == 1:\n",
    "        pred_evidence = ss\n",
    "  else:\n",
    "    pred_evidence = -1 # This should never happen - it would be inconsistent if it did\n",
    "  preds_evidence.append(pred_evidence)\n",
    "\n",
    "# Calculate tiered accuracy for model\n",
    "acc = 0\n",
    "acc_con = 0\n",
    "acc_con_vbp = 0\n",
    "acc_con_vbp_vev = 0\n",
    "no_ex = len(ids_base)\n",
    "for p_plaus, l_plaus, con, p_bp, l_bp, p_ev, l_ev in zip(preds_entailment, labels_entailment, preds_consistent, preds_breakpoint, labels_breakpoint, preds_evidence, labels_evidence):\n",
    "  # Accuracy\n",
    "  if p_plaus == l_plaus:\n",
    "    acc += 1\n",
    "    \n",
    "    # Consistency\n",
    "    if con == 1:\n",
    "      acc_con += 1\n",
    "    \n",
    "      # Verifiability (breakpoint)\n",
    "      if p_bp == l_bp:\n",
    "        acc_con_vbp += 1\n",
    "\n",
    "        # Verifiability (evidence)\n",
    "        if p_ev == l_ev:\n",
    "          acc_con_vbp_vev += 1\n",
    "\n",
    "acc /= no_ex\n",
    "acc_con /= no_ex\n",
    "acc_con_vbp /= no_ex\n",
    "acc_con_vbp_vev /= no_ex\n",
    "\n",
    "# all_results['acc'] = acc\n",
    "# all_results['acc_con'] = acc_con\n",
    "# all_results['acc_con_vbp'] = acc_con_vbp\n",
    "# all_results['acc_con_vbp_vev'] = acc_con_vbp_vev\n",
    "# all_results['span_accuracy'] = np.mean(span_accuracies)\n",
    "\n",
    "all_results['lenient_coherence'] = np.mean(span_accuracies_strict)\n",
    "all_results['strict_coherence'] = np.mean(preds_coherent)\n",
    "\n",
    "best_preds_entailment = preds_entailment\n",
    "best_preds_consistent = preds_consistent\n",
    "best_preds_breakpoint = preds_breakpoint\n",
    "best_preds_evidence = preds_evidence\n",
    "best_preds_coherent = preds_coherent\n",
    "    \n",
    "print('\\nPARTITION: %s' % p)\n",
    "print_dict(all_results)\n",
    "\n",
    "# Save preds for breakpoint and evidence\n",
    "save_preds(ids_base, np.array(labels_breakpoint), best_preds_breakpoint, probe_model, p_dataset_name_bp)\n",
    "save_preds(ids_base, np.array(labels_evidence), best_preds_evidence, probe_model, p_dataset_name_ev)\n",
    "save_preds(ids_base, np.array([1 for p in best_preds_coherent]), best_preds_coherent, probe_model, p_dataset_name_coh)\n",
    "\n",
    "p_dataset_name_agg = '%s_tiers_agg_nostates_lenient_%s' % ('ConvEnt', p)\n",
    "save_results(all_results, probe_model, p_dataset_name_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnWDvQG7vtv-"
   },
   "source": [
    "# ART Results\n",
    "\n",
    "Code for the coherence experiments on ART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvqFFHXebfrI"
   },
   "outputs": [],
   "source": [
    "if task_name != 'art':\n",
    "  raise ValueError('Please configure task_name in first cell to \"art\" to run ART results!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRnuX_fvtv_"
   },
   "source": [
    "## Load ART dataset\n",
    "\n",
    "ART is originally gathered from [HuggingFace datasets](https://huggingface.co/docs/datasets/), but we added some of our own annotations for the coherence evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGZGEObJvtv_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "fname = os.path.join(DRIVE_PATH, 'all_data/ART/art.json')\n",
    "with open(fname, 'r') as f:\n",
    "  art = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apGoBEp_vtv_"
   },
   "source": [
    "## Train Models on ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukLFJ-Mfvtv_"
   },
   "source": [
    "### Featurize ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LbWyaWKvtv_"
   },
   "outputs": [],
   "source": [
    "from www.dataset.featurize import add_bert_features_art, get_tensor_dataset\n",
    "seq_length = 32\n",
    "\n",
    "for p in art:\n",
    "  for i in range(len(art[p])):\n",
    "    art[p][i]['label'] -= 1 # Do this so labels start at 0\n",
    "\n",
    "  if debug:\n",
    "    # Take 20 examples that we've annotated as the debug set so we can run the coherence metrics\n",
    "    merged_file = os.path.join(DRIVE_PATH, 'all_data/ART/ART_test_rand200_annotation_merged2.json')\n",
    "    ann_ids = [ex['id'] for ex in json.load(open(merged_file))]\n",
    "     \n",
    "    if p == 'train':\n",
    "      art[p] = art[p][:20]\n",
    "      art[p] = art[p][:20]\n",
    "    elif p == 'val':\n",
    "      art[p] = [ex for ex in art[p] if ex['id'] in ann_ids][:20]\n",
    "\n",
    "art_tensor = {}\n",
    "for p in art:\n",
    "  art[p] = add_bert_features_art(art[p], tokenizer, seq_length)\n",
    "  art_tensor[p] = get_tensor_dataset(art[p], label_key='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XERMrM56vtv_"
   },
   "source": [
    "### Train Models\n",
    "\n",
    "Train models on ART. Note that ART's test set is not public, so we cannot test the model (unless we submit to their [leaderboard](https://leaderboard.allenai.org/anli/submissions/public))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u4dOjy4vtwA"
   },
   "source": [
    "#### Configure Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz3-D5hVvtwA"
   },
   "outputs": [],
   "source": [
    "batch_sizes = [config_batch_size]\n",
    "learning_rates = [config_lr]\n",
    "epochs = config_epochs\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-cs-hhJvtwA"
   },
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28687,
     "status": "ok",
     "timestamp": 1631304042954,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "wRfuT81qvtwA",
    "outputId": "e6e8ad02-9836-49e8-b0f3-615960aa5d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning grid search for ART over 1 parameter combination(s)!\n",
      "\n",
      "TRAINING MODEL: bs=1, lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Beginning epoch...\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:00s.\n",
      "[0] Validation results:\n",
      "{\n",
      "  accuracy: \n",
      "    0.7,\n",
      "}\n",
      "\n",
      "\n",
      "[0] Saving model checkpoint...\n",
      "[0] Finished epoch.\n",
      "Finished grid search! :)\n",
      "Best validation accuracy 0.7 from model roberta-large_art_1_1e-05_0.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup, RobertaForMultipleChoice, BertForMultipleChoice, RobertaConfig, BertConfig\n",
    "from www.model.train import train_epoch\n",
    "from www.model.eval import evaluate, save_results, save_preds\n",
    "from sklearn.metrics import accuracy_score\n",
    "from www.utils import print_dict, get_model_dir\n",
    "\n",
    "seed_val = 22 # Save random seed for reproducibility\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll keep the validation data here with a constant eval batch size\n",
    "dev_sampler = SequentialSampler(art_tensor['val'])\n",
    "dev_dataloader = DataLoader(art_tensor['val'], sampler=dev_sampler, batch_size=eval_batch_size)\n",
    "dev_dataset_name = 'art_val'\n",
    "dev_ids = [str(ex['example_id']) for ex in art['val']]\n",
    "\n",
    "all_losses = []\n",
    "param_combos = []\n",
    "combo_names = []\n",
    "all_val_accs = []\n",
    "output_dirs = []\n",
    "best_acc = 0.0\n",
    "\n",
    "print('Beginning grid search for ART over %s parameter combination(s)!' % (str(len(batch_sizes) * len(learning_rates))))\n",
    "for bs in batch_sizes:\n",
    "  for lr in learning_rates:\n",
    "    print('\\nTRAINING MODEL: bs=%s, lr=%s' % (str(bs), str(lr)))\n",
    "\n",
    "    loss_values = []\n",
    "    acc_values = []\n",
    "\n",
    "    # Set up training dataset with new batch size\n",
    "    train_sampler = RandomSampler(art_tensor['train'])\n",
    "    train_dataloader = DataLoader(art_tensor['train'], sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "    # Set up model\n",
    "    config = config_class.from_pretrained(model_name,\n",
    "                                          num_labels=2,\n",
    "                                          cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "    model = model_class.from_pretrained(model_name,\n",
    "                                        config=config,\n",
    "                                        cache_dir=os.path.join(DRIVE_PATH, 'cache'))\n",
    "\n",
    "    model.cuda()\n",
    "    device = model.device \n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      # Train the model for one epoch\n",
    "      print('[%s] Beginning epoch...' % str(epoch))\n",
    "\n",
    "      epoch_loss, _ = train_epoch(model, optimizer, train_dataloader, device)\n",
    "      \n",
    "      # Save loss\n",
    "      loss_values.append(epoch_loss)\n",
    "\n",
    "      # Validate on dev set\n",
    "      results, preds, labels = evaluate(model, dev_dataloader, device, [(accuracy_score, 'accuracy')])\n",
    "      print('[%s] Validation results:' % str(epoch))\n",
    "      print_dict(results)\n",
    "\n",
    "      # Save accuracy\n",
    "      acc = results['accuracy']\n",
    "      acc_values.append(acc)\n",
    "      \n",
    "      # Save model checkpoint\n",
    "      print('[%s] Saving model checkpoint...' % str(epoch))\n",
    "      model_param_str = get_model_dir(model_name.replace('/','-'), 'art', bs, lr, epoch)# + '_toy'\n",
    "      output_dir = os.path.join(DRIVE_PATH, 'saved_models', model_param_str)\n",
    "      output_dirs.append(output_dir)\n",
    "      if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "      save_results(results, output_dir, dev_dataset_name)\n",
    "      save_preds(dev_ids, labels, preds, output_dir, dev_dataset_name)\n",
    "      model = model.module if hasattr(model, 'module') else model\n",
    "      model.save_pretrained(output_dir)\n",
    "      tokenizer.save_vocabulary(output_dir)\n",
    "\n",
    "      if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model_param_str\n",
    "        best_dir = output_dir\n",
    "\n",
    "      print('[%s] Finished epoch.' % str(epoch))\n",
    "\n",
    "    all_losses.append(loss_values)\n",
    "    all_val_accs.append(acc_values)\n",
    "    param_combos.append((bs, lr))\n",
    "    combo_names.append('bs=%s, lr=%s' % (str(bs), str(lr)))\n",
    "\n",
    "print('Finished grid search! :)')\n",
    "print('Best validation accuracy %s from model %s.' % (best_acc, best_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPSVjSw9vtwB"
   },
   "source": [
    "Delete non-best model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-Pqf4g1vtwB"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Delete non-best model checkpoints\n",
    "for od in output_dirs:\n",
    "  if od != best_dir and os.path.exists(od):\n",
    "    shutil.rmtree(od)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrNuHxTEvtwB"
   },
   "source": [
    "## Coherence Checks on ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egyx9BejvtwC"
   },
   "source": [
    "### Load and Featurize Span Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mD8HXC9vtwC"
   },
   "outputs": [],
   "source": [
    "from www.dataset.featurize import add_bert_features_art, get_tensor_dataset\n",
    "from www.dataset.prepro import get_art_spans\n",
    "import pickle\n",
    "seq_length = 128\n",
    "  \n",
    "merged_file = os.path.join(DRIVE_PATH, 'all_data/ART/ART_test_rand200_annotation_merged2.json')\n",
    "art_anns = json.load(open(merged_file))\n",
    "\n",
    "if debug:\n",
    "  ann_ids = [ex['id'] for ex in art_anns]\n",
    "  debug_ids = [ex['id'] for ex in art[p] if ex['id'] in ann_ids][:20]  \n",
    "  art = [ex for ex in art_anns if ex['id'] in debug_ids]\n",
    "\n",
    "# Make span versions of the datasets\n",
    "art_spans = get_art_spans(art)\n",
    "\n",
    "# Add BERT features\n",
    "art = add_bert_features_art(art, tokenizer, seq_length, add_segment_ids=True)\n",
    "art_spans = add_bert_features_art(art_spans, tokenizer, seq_length, add_segment_ids=True)\n",
    "\n",
    "# Add BERT features\n",
    "art_tensor = get_tensor_dataset(art, label_key='label', add_segment_ids=True)\n",
    "art_spans_tensor = get_tensor_dataset(art_spans, label_key='label', add_segment_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auYYxFc6vtwC"
   },
   "source": [
    "### Load the Trained Model\n",
    "\n",
    "Load the trained model we want to probe and select the appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruiy_uXqvtwC"
   },
   "outputs": [],
   "source": [
    "probe_model = eval_model_dir\n",
    "probe_model = os.path.join(DRIVE_PATH, 'saved_models', probe_model)\n",
    "  \n",
    "# Load the model\n",
    "model = model_class.from_pretrained(probe_model)\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()\n",
    "device = model.device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwznwL-MvtwC"
   },
   "source": [
    "#### Load Trained Model's Two-Story Classification Predictions\n",
    "\n",
    "For comparison, we also want the preds and labels for the previous level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdIPP8oPvtwC"
   },
   "outputs": [],
   "source": [
    "from www.model.eval import load_preds\n",
    "from www.utils import print_dict\n",
    "\n",
    "preds_base = {}\n",
    "preds_base['val'] = load_preds(os.path.join(probe_model, 'preds_art_val.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYRfQG6LvtwC"
   },
   "source": [
    "### Calculate Coherence Metrics\n",
    "\n",
    "As ART is a multiple-choice task, we will need to tune the confidence threshold $\\rho$. This code will print out the strict and lenient coherence metrics, as well as the chosen $\\rho$ (`best_threshold`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3496,
     "status": "ok",
     "timestamp": 1631304420628,
     "user": {
      "displayName": "Shane Storks",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh8CqjwUclCWWIoFwUB05rZNHwyd6nEIrTXC6RVXw=s64",
      "userId": "00590987742306216924"
     },
     "user_tz": 240
    },
    "id": "dCfpfrDXvtwD",
    "outputId": "3d4bf905-11f1-41a9-889b-cb55114e67bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: drive/My Drive/Colab Notebooks/Research/TRIP_replication/saved_models/roberta-large_art_1_1e-05_0.\n",
      "\tBeginning evaluation...\n",
      "\t\tRunning prediction...\n",
      "\t\tComputing metrics...\n",
      "\tFinished evaluation in 0:00:03s.\n",
      "\n",
      "PARTITION: val \t METRIC: strict_coherence\n",
      "chosen threshold: 1.0\n",
      "{\n",
      "  lenient_coherence: \n",
      "    0.18246427120454473,\n",
      "  strict_coherence: \n",
      "    0.15,\n",
      "  best_threshold: \n",
      "    1.0,\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "PARTITION: val \t METRIC: lenient_coherence\n",
      "chosen threshold: 1.0\n",
      "{\n",
      "  lenient_coherence: \n",
      "    0.18246427120454473,\n",
      "  strict_coherence: \n",
      "    0.15,\n",
      "  best_threshold: \n",
      "    1.0,\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from www.model.eval import evaluate, save_results, save_preds, list_comparison\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "metrics = [(accuracy_score, 'accuracy'), (precision_score, 'precision'), (recall_score, 'recall'), (f1_score, 'f1')]\n",
    "import numpy as np\n",
    "from www.utils import print_dict\n",
    "\n",
    "def is_polarized(smax, thres):\n",
    "  return (abs(smax[0] - smax[1]) >= thres)\n",
    "\n",
    "print('Testing model: %s.' % probe_model)\n",
    "\n",
    "subtask = 'art'\n",
    "p = 'val'\n",
    "all_results = {}\n",
    "\n",
    "p_dataset = art_spans\n",
    "p_tensor_dataset = art_spans_tensor\n",
    "p_sampler = SequentialSampler(p_tensor_dataset)\n",
    "p_dataloader = DataLoader(p_tensor_dataset, sampler=p_sampler, batch_size=128)\n",
    "p_dataset_name = '%s_spans_%s' % (subtask, p)\n",
    "p_dataset_name_co = '%s_consistent_%s' % (subtask, p)\n",
    "p_dataset_name_bp = '%s_breakpoints_%s' % (subtask, p)\n",
    "p_dataset_name_ev = '%s_evidence_%s' % (subtask, p)\n",
    "p_dataset_name_coh = '%s_coherence_%s' % (subtask, p)\n",
    "p_dataset_name_subset = '%s_rand200_%s' % (subtask, p)\n",
    "p_ids = [ex['example_id'] for ex in art_spans]\n",
    "p_labels = [ex['label'] for ex in art_spans]\n",
    "\n",
    "# Get span preds and save metrics\n",
    "results, preds, labels, probs = evaluate(model, p_dataloader, device, metrics, seg_mode=True if 'roberta' not in mode else False, return_softmax=True)\n",
    "save_results(results, probe_model, p_dataset_name)\n",
    "save_preds(p_ids, labels, preds, probe_model, p_dataset_name)\n",
    "\n",
    "# Convert substory preds into breakpoint preds for each example\n",
    "ids_base = [ex['example_id'] for ex in art]\n",
    "\n",
    "id_to_pred = {k: v for k,v in zip(p_ids, preds)}\n",
    "id_to_prob = {k: v for k,v in zip(p_ids, probs)}\n",
    "id_to_label = {k: v for k,v in zip(p_ids, p_labels)}\n",
    "\n",
    "for metric_to_optimize in ['strict_coherence', 'lenient_coherence']:\n",
    "  # Get results dict ready\n",
    "  # all_results['acc'] = 0.0\n",
    "  # all_results['acc_con'] = 0.0\n",
    "  # all_results['acc_con_vbp'] = 0.0\n",
    "  # all_results['acc_con_vbp_vev'] = 0.0\n",
    "  # all_results['span_accuracy'] = 0.0\n",
    "  all_results['lenient_coherence'] = 0.0\n",
    "  all_results['strict_coherence'] = 0.0\n",
    "  span_accuracy = 0.0\n",
    "  span_accuracy_strict = 0.0\n",
    "  no_spans = 0\n",
    "  for threshold in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n",
    "\n",
    "    preds_plausible = []\n",
    "    labels_plausible = []\n",
    "    preds_consistent = []\n",
    "    preds_breakpoint = []\n",
    "    labels_breakpoint = []\n",
    "    preds_evidence = []\n",
    "    labels_evidence = []    \n",
    "    span_accuracies = []\n",
    "    span_accuracies_strict = []\n",
    "    preds_coherent = []\n",
    "\n",
    "    for i, exid in enumerate(ids_base):\n",
    "      ex = art[i]\n",
    "      ex['length'] = 3\n",
    "\n",
    "      label_plausible = preds_base[p][exid]['label']\n",
    "      pred_plausible = preds_base[p][exid]['pred']\n",
    "      labels_plausible.append(label_plausible)\n",
    "      preds_plausible.append(pred_plausible)\n",
    "\n",
    "      # Get ground truth breakpoint and evidence\n",
    "      label_breakpoint = ex['conflict_pair'][1] if ex['conflict_pair'] is not None else 0\n",
    "      labels_breakpoint.append(label_breakpoint)\n",
    "      if label_breakpoint > 0:\n",
    "        label_ev = ex['conflict_pair'][0]\n",
    "      else:\n",
    "        label_ev = -1\n",
    "      labels_evidence.append(label_ev)\n",
    "\n",
    "      # Check consistency - for every span we confidently choose story X, we should also confidently choose story X for any span containing it\n",
    "      pred_consistent = True\n",
    "      for sp1 in range(ex['length']-1):\n",
    "        if not pred_consistent:\n",
    "          break\n",
    "\n",
    "        for sp2 in range(sp1+1, ex['length']):\n",
    "          if not pred_consistent:\n",
    "            break\n",
    "\n",
    "          span_pred = int(id_to_pred[exid + '-sp%s:%s' % (str(sp1), str(sp2))])\n",
    "          span_prob = id_to_prob[exid + '-sp%s:%s' % (str(sp1), str(sp2))]\n",
    "          span_label = max(id_to_label[exid + '-sp%s:%s' % (str(sp1), str(sp2))] - 1, -1)\n",
    "\n",
    "          span_pred3 = span_pred\n",
    "          if not is_polarized(span_prob, threshold): # If not polarized, let's say -1\n",
    "            span_pred3 = -1\n",
    "\n",
    "          pred_coherent = True\n",
    "          if span_pred3 == span_label:\n",
    "            span_accuracy += 1.0\n",
    "            if label_plausible == pred_plausible:\n",
    "              span_accuracy_strict += 1.0\n",
    "          else:\n",
    "            pred_coherent = False\n",
    "          no_spans += 1\n",
    "\n",
    "          if is_polarized(span_prob, threshold):\n",
    "            for sp3 in range(sp1+1):\n",
    "              if not pred_consistent:\n",
    "                break\n",
    "\n",
    "              for sp4 in range(sp2, ex['length']):\n",
    "                if not pred_consistent:\n",
    "                  break\n",
    "\n",
    "                sspan_pred = id_to_pred[exid + '-sp%s:%s' % (str(sp3), str(sp4))]\n",
    "                sspan_prob = id_to_prob[exid + '-sp%s:%s' % (str(sp3), str(sp4))]\n",
    "\n",
    "                if not is_polarized(sspan_prob, threshold) or sspan_pred != span_pred:\n",
    "                  pred_consistent = False\n",
    "                  break\n",
    "\n",
    "      preds_consistent.append(1 if pred_consistent else 0)\n",
    "      span_accuracies.append(span_accuracy / no_spans)\n",
    "      span_accuracies_strict.append(span_accuracy_strict / no_spans)\n",
    "      preds_coherent.append(1 if pred_coherent else 0)\n",
    "\n",
    "      # Check pred. breakpoint (verifiability) - will be first sentence where the model prediction becomes polarized, i.e., confidence > threshold\n",
    "      pred_breakpoint  = 0 # For now, 0 means -1, i.e., stories are entirely plausible - this shouldn't happen but it will (inconsistent?)\n",
    "      for ss in range(1, ex['length']):\n",
    "        if is_polarized(id_to_prob[exid + '-sp%s:%s' % (str(0), str(ss))], threshold):\n",
    "          pred_breakpoint = ss\n",
    "          break\n",
    "      preds_breakpoint.append(pred_breakpoint)\n",
    "\n",
    "      # Check pred. evidence (verifiability)\n",
    "      if pred_breakpoint > 0:\n",
    "        pred_evidence = -1 # Does this make sense for default value?\n",
    "        for ss in range(0, pred_breakpoint):\n",
    "          if is_polarized(id_to_prob[exid + '-sp%s:%s' % (str(ss), str(pred_breakpoint))], threshold):\n",
    "            pred_evidence = ss\n",
    "      else:\n",
    "        pred_evidence = -1 # This should never happen - it would be inconsistent if it did?\n",
    "      preds_evidence.append(pred_evidence)\n",
    "\n",
    "    # Calculate tiered accuracy for model\n",
    "    acc = 0\n",
    "    acc_con = 0\n",
    "    acc_con_vbp = 0\n",
    "    acc_con_vbp_vev = 0\n",
    "    no_ex = len(ids_base)\n",
    "    for p_plaus, l_plaus, con, p_bp, l_bp, p_ev, l_ev in zip(preds_plausible, labels_plausible, preds_consistent, preds_breakpoint, labels_breakpoint, preds_evidence, labels_evidence):\n",
    "      # Accuracy\n",
    "      if p_plaus == l_plaus:\n",
    "        acc += 1\n",
    "        \n",
    "        # Consistency\n",
    "        if con == 1:\n",
    "          acc_con += 1\n",
    "        \n",
    "          # Verifiability (breakpoint)\n",
    "          if p_bp == l_bp:\n",
    "            acc_con_vbp += 1\n",
    "\n",
    "            # Verifiability (evidence)\n",
    "            if p_ev == l_ev:\n",
    "              acc_con_vbp_vev += 1\n",
    "\n",
    "    acc /= no_ex\n",
    "    acc_con /= no_ex\n",
    "    acc_con_vbp /= no_ex\n",
    "    acc_con_vbp_vev /= no_ex\n",
    "    span_acc = np.mean(span_accuracies)\n",
    "    span_acc_strict = np.mean(span_accuracies_strict)\n",
    "    coherence = np.mean(preds_coherent)\n",
    "    # if coherence > all_results['coherence']: # !!!! this line is important\n",
    "    # if span_acc > all_results['span_accuracy']: # !!!! this line is important\n",
    "    if span_acc_strict > all_results[metric_to_optimize]: # !!!! this line is important\n",
    "      # print('new best: %s' % str(acc_con_vbp_vev))\n",
    "      best_thres = threshold\n",
    "      \n",
    "      # all_results['acc'] = acc\n",
    "      # all_results['acc_con'] = acc_con\n",
    "      # all_results['acc_con_vbp'] = acc_con_vbp\n",
    "      # all_results['acc_con_vbp_vev'] = acc_con_vbp_vev\n",
    "      # all_results['span_accuracy'] = span_acc\n",
    "      all_results['lenient_coherence'] = span_acc_strict\n",
    "      all_results['strict_coherence'] = coherence\n",
    "\n",
    "      best_preds_plausible = preds_plausible\n",
    "      best_preds_consistent = preds_consistent\n",
    "      best_preds_breakpoint = preds_breakpoint\n",
    "      best_preds_evidence = preds_evidence\n",
    "      best_preds_coherent = preds_coherent\n",
    "      \n",
    "  all_results['best_threshold'] = best_thres\n",
    "  print('\\nPARTITION: %s \\t METRIC: %s' % (p, metric_to_optimize))\n",
    "  print('chosen threshold: %s' % str(best_thres))\n",
    "  print_dict(all_results)\n",
    "\n",
    "  # Save results\n",
    "  p_dataset_name_agg = '%s_%s_%s' % (subtask, metric_to_optimize, p)\n",
    "  save_results(all_results, probe_model, p_dataset_name_agg)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN086hRntugR7AhDFlEbzLv",
   "collapsed_sections": [
    "XL9AU7zTgP9n",
    "qqvj34KhLL0k",
    "xm7qzKnAnbU9",
    "FoY37xIF-oP7",
    "QzFnAVtuUmpQ",
    "lKY0hTEgnQgB",
    "k_0WsycpFMdb",
    "Cz5tcmScJrka",
    "BxIYaEobhR7J",
    "v7VlN2jUwvcC",
    "5ctQweSlAceo",
    "0q-xjfYU_cV8",
    "-fQ6wXQIBdq1",
    "U-BMInyrBdq2",
    "8fRC3cnLBdq3",
    "aWFmGRhznl2T",
    "Rbvpm9irn3qL",
    "RfXiCTA9KPjG",
    "ON1UAnbc8OOE",
    "W7MADzgfvtv3",
    "Msxt3xAhvtv6",
    "rn5Ywwvxvtv6",
    "Ytbj9Uxxvtv7",
    "h9YL5qnRvtv7",
    "Jss8T2xzvtv8",
    "8XhSrDlpI0aH",
    "CADDFieTvtv9",
    "maZGCMvMvtv-",
    "XnWDvQG7vtv-",
    "KxRnuX_fvtv_",
    "apGoBEp_vtv_",
    "ukLFJ-Mfvtv_",
    "XERMrM56vtv_",
    "Egyx9BejvtwC",
    "auYYxFc6vtwC",
    "fYRfQG6LvtwC"
   ],
   "machine_shape": "hm",
   "mount_file_id": "1RD-qTvUOr5V5r5QXWS4qCGVx2qM96ur2",
   "name": "(code release) Consistent, Verifiable, and Coherent Reasoning for LMs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
